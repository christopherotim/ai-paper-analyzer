[
    {
        "id": "2507.19849",
        "title": "Agentic Reinforced Policy Optimization",
        "translation": "Agentic Reinforced Policy Optimization",
        "url": "https://arxiv.org/abs/2507.19849",
        "authors": "Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
        "publish_date": "2025-07-26T07:53:11.000Z",
        "summary": "Large-scale reinforcement learning with verifiable rewards (RLVR) has\ndemonstrated its effectiveness in harnessing the potential of large language\nmodels (LLMs) for single-turn reasoning tasks. In realistic reasoning\nscenarios, LLMs can often utilize external tools to assist in task-solving\nprocesses. However, current RL algorithms inadequately balance the models'\nintrinsic long-horizon reasoning capabilities and their proficiency in\nmulti-turn tool interactions. To bridge this gap, we propose Agentic Reinforced\nPolicy Optimization (ARPO), a novel agentic RL algorithm tailored for training\nmulti-turn LLM-based agents. Through preliminary experiments, we observe that\nLLMs tend to exhibit highly uncertain behavior, characterized by an increase in\nthe entropy distribution of generated tokens, immediately following\ninteractions with external tools. Motivated by this observation, ARPO\nincorporates an entropy-based adaptive rollout mechanism, dynamically balancing\nglobal trajectory sampling and step-level sampling, thereby promoting\nexploration at steps with high uncertainty after tool usage. By integrating an\nadvantage attribution estimation, ARPO enables LLMs to internalize advantage\ndifferences in stepwise tool-use interactions. Our experiments across 13\nchallenging benchmarks in computational reasoning, knowledge reasoning, and\ndeep search domains demonstrate ARPO's superiority over trajectory-level RL\nalgorithms. Remarkably, ARPO achieves improved performance using only half of\nthe tool-use budget required by existing methods, offering a scalable solution\nfor aligning LLM-based agents with real-time dynamic environments. Our code and\ndatasets are released at https://github.com/dongguanting/ARPO",
        "github_repo": "https://github.com/dongguanting/ARPO",
        "project_page": "https://github.com/dongguanting/ARPO",
        "model_function": ""
    },
    {
        "id": "2507.21046",
        "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super   Intelligence",
        "translation": "A Survey of Self-Evolving Agents: On Path to Artificial Super   Intelligence",
        "url": "https://arxiv.org/abs/2507.21046",
        "authors": "Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang",
        "publish_date": "2025-07-28T17:59:05.000Z",
        "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
        "github_repo": "https://github.com/CharlesQ9/Self-Evolving-Agents",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20939",
        "title": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World   Shorts",
        "translation": "ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World   Shorts",
        "url": "https://arxiv.org/abs/2507.20939",
        "authors": "Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, Jinwen Luo, Weibo Gu, Zexuan Li, Xiaojing Zhang, Yangyu Tao, Han Hu, Di Wang, Ying Shan",
        "publish_date": "2025-07-28T15:52:36.000Z",
        "summary": "Real-world user-generated short videos, especially those distributed on\nplatforms such as WeChat Channel and TikTok, dominate the mobile internet.\nHowever, current large multimodal models lack essential temporally-structured,\ndetailed, and in-depth video comprehension capabilities, which are the\ncornerstone of effective video search and recommendation, as well as emerging\nvideo applications. Understanding real-world shorts is actually challenging due\nto their complex visual elements, high information density in both visuals and\naudio, and fast pacing that focuses on emotional expression and viewpoint\ndelivery. This requires advanced reasoning to effectively integrate multimodal\ninformation, including visual, audio, and text. In this work, we introduce\nARC-Hunyuan-Video, a multimodal model that processes visual, audio, and textual\nsignals from raw video inputs end-to-end for structured comprehension. The\nmodel is capable of multi-granularity timestamped video captioning and\nsummarization, open-ended video question answering, temporal video grounding,\nand video reasoning. Leveraging high-quality data from an automated annotation\npipeline, our compact 7B-parameter model is trained through a comprehensive\nregimen: pre-training, instruction fine-tuning, cold start, reinforcement\nlearning (RL) post-training, and final instruction fine-tuning. Quantitative\nevaluations on our introduced benchmark ShortVid-Bench and qualitative\ncomparisons demonstrate its strong performance in real-world video\ncomprehension, and it supports zero-shot or fine-tuning with a few samples for\ndiverse downstream applications. The real-world production deployment of our\nmodel has yielded tangible and measurable improvements in user engagement and\nsatisfaction, a success supported by its remarkable efficiency, with stress\ntests indicating an inference time of just 10 seconds for a one-minute video on\nH20 GPU.",
        "github_repo": "https://github.com/TencentARC/ARC-Hunyuan-Video-7B",
        "project_page": "https://tencentarc.github.io/posts/arc-video-announcement/",
        "model_function": ""
    },
    {
        "id": "2507.20984",
        "title": "SmallThinker: A Family of Efficient Large Language Models Natively   Trained for Local Deployment",
        "translation": "SmallThinker: A Family of Efficient Large Language Models Natively   Trained for Local Deployment",
        "url": "https://arxiv.org/abs/2507.20984",
        "authors": "Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen",
        "publish_date": "2025-07-28T16:45:14.000Z",
        "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21049",
        "title": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for   Multi-Task Learning",
        "translation": "Rep-MTL: Unleashing the Power of Representation-level Task Saliency for   Multi-Task Learning",
        "url": "https://arxiv.org/abs/2507.21049",
        "authors": "Zedong Wang, Siyuan Li, Dan Xu",
        "publish_date": "2025-07-28T17:59:28.000Z",
        "summary": "Despite the promise of Multi-Task Learning in leveraging complementary\nknowledge across tasks, existing multi-task optimization (MTO) techniques\nremain fixated on resolving conflicts via optimizer-centric loss scaling and\ngradient manipulation strategies, yet fail to deliver consistent gains. In this\npaper, we argue that the shared representation space, where task interactions\nnaturally occur, offers rich information and potential for operations\ncomplementary to existing optimizers, especially for facilitating the\ninter-task complementarity, which is rarely explored in MTO. This intuition\nleads to Rep-MTL, which exploits the representation-level task saliency to\nquantify interactions between task-specific optimization and shared\nrepresentation learning. By steering these saliencies through entropy-based\npenalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate\nnegative transfer by maintaining the effective training of individual tasks\ninstead pure conflict-solving, while explicitly promoting complementary\ninformation sharing. Experiments are conducted on four challenging MTL\nbenchmarks covering both task-shift and domain-shift scenarios. The results\nshow that Rep-MTL, even paired with the basic equal weighting policy, achieves\ncompetitive performance gains with favorable efficiency. Beyond standard\nperformance metrics, Power Law exponent analysis demonstrates Rep-MTL's\nefficacy in balancing task-specific learning and cross-task sharing. The\nproject page is available at HERE.",
        "github_repo": "https://github.com/Jacky1128/Rep-MTL",
        "project_page": "https://jacky1128.github.io/RepMTL/",
        "model_function": ""
    },
    {
        "id": "2507.21045",
        "title": "Reconstructing 4D Spatial Intelligence: A Survey",
        "translation": "Reconstructing 4D Spatial Intelligence: A Survey",
        "url": "https://arxiv.org/abs/2507.21045",
        "authors": "Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu",
        "publish_date": "2025-07-28T17:59:02.000Z",
        "summary": "Reconstructing 4D spatial intelligence from visual observations has long been\na central yet challenging task in computer vision, with broad real-world\napplications. These range from entertainment domains like movies, where the\nfocus is often on reconstructing fundamental visual elements, to embodied AI,\nwhich emphasizes interaction modeling and physical realism. Fueled by rapid\nadvances in 3D representations and deep learning architectures, the field has\nevolved quickly, outpacing the scope of previous surveys. Additionally,\nexisting surveys rarely offer a comprehensive analysis of the hierarchical\nstructure of 4D scene reconstruction. To address this gap, we present a new\nperspective that organizes existing methods into five progressive levels of 4D\nspatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes\n(e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene\ncomponents (e.g., objects, humans, structures); (3) Level 3 -- reconstruction\nof 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene\ncomponents; and (5) Level 5 -- incorporation of physical laws and constraints.\nWe conclude the survey by discussing the key challenges at each level and\nhighlighting promising directions for advancing toward even richer levels of 4D\nspatial intelligence. To track ongoing developments, we maintain an up-to-date\nproject page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.",
        "github_repo": "https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20673",
        "title": "Geometric-Mean Policy Optimization",
        "translation": "Geometric-Mean Policy Optimization",
        "url": "https://arxiv.org/abs/2507.20673",
        "authors": "Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei",
        "publish_date": "2025-07-28T09:54:05.000Z",
        "summary": "Recent advancements, such as Group Relative Policy Optimization (GRPO), have\nenhanced the reasoning capabilities of large language models by optimizing the\narithmetic mean of token-level rewards. However, GRPO suffers from unstable\npolicy updates when processing tokens with outlier importance-weighted rewards,\nwhich manifests as extreme importance sampling ratios during training, i.e.,\nthe ratio between the sampling probabilities assigned to a token by the current\nand old policies. In this work, we propose Geometric-Mean Policy Optimization\n(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic\nmean, GMPO maximizes the geometric mean of token-level rewards, which is\ninherently less sensitive to outliers and maintains a more stable range of\nimportance sampling ratio. In addition, we provide comprehensive theoretical\nand experimental analysis to justify the design and stability benefits of GMPO.\nBeyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on\nmultiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,\nincluding AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is\navailable at https://github.com/callsys/GMPO.",
        "github_repo": "https://github.com/callsys/GMPO",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20187",
        "title": "Diversity-Enhanced Reasoning for Subjective Questions",
        "translation": "Diversity-Enhanced Reasoning for Subjective Questions",
        "url": "https://arxiv.org/abs/2507.20187",
        "authors": "Yumeng Wang, Zhiyuan Fan, Jiayu Liu, Yi R. Fung",
        "publish_date": "2025-07-27T09:07:42.000Z",
        "summary": "Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities\nhave shown strong performance on objective tasks, such as math reasoning and\ncoding. However, their effectiveness on subjective questions that may have\ndifferent responses from different perspectives is still limited by a tendency\ntowards homogeneous reasoning, introduced by the reliance on a single ground\ntruth in supervised fine-tuning and verifiable reward in reinforcement\nlearning. Motivated by the finding that increasing role perspectives\nconsistently improves performance, we propose MultiRole-R1, a\ndiversity-enhanced framework with multiple role perspectives, to improve the\naccuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an\nunsupervised data construction pipeline that generates reasoning chains that\nincorporate diverse role perspectives. We further employ reinforcement learning\nvia Group Relative Policy Optimization (GRPO) with reward shaping, by taking\ndiversity as a reward signal in addition to the verifiable reward. With\nspecially designed reward functions, we successfully promote perspective\ndiversity and lexical diversity, uncovering a positive relation between\nreasoning diversity and accuracy. Our experiment on six benchmarks demonstrates\nMultiRole-R1's effectiveness and generalizability in enhancing both subjective\nand objective reasoning, showcasing the potential of diversity-enhanced\ntraining in LRMs.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21033",
        "title": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
        "translation": "GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset",
        "url": "https://arxiv.org/abs/2507.21033",
        "authors": "Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie",
        "publish_date": "2025-07-28T17:54:04.000Z",
        "summary": "Recent advancements in large multimodal models like GPT-4o have set a new\nstandard for high-fidelity, instruction-guided image editing. However, the\nproprietary nature of these models and their training data creates a\nsignificant barrier for open-source research. To bridge this gap, we introduce\nGPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus\ncontaining more than 1.5 million high-quality triplets (instruction, source\nimage, edited image). We systematically construct this dataset by leveraging\nthe versatile capabilities of GPT-4o to unify and refine three popular\nimage-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our\nmethodology involves 1) regenerating output images to enhance visual quality\nand instruction alignment, and 2) selectively rewriting prompts to improve\nsemantic clarity. To validate the efficacy of our dataset, we fine-tune\nadvanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are\nexciting, e.g., the fine-tuned FluxKontext achieves highly competitive\nperformance across a comprehensive suite of benchmarks, including 7.24 on\nGEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger\ninstruction following and higher perceptual quality while maintaining identity.\nThese scores markedly exceed all previously published open-source methods and\nsubstantially narrow the gap to leading proprietary models. We hope the full\nrelease of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in\ninstruction-guided image editing.",
        "github_repo": "https://github.com/wyhlovecpp/GPT-Image-Edit",
        "project_page": "https://ucsc-vlaa.github.io/GPT-Image-Edit/",
        "model_function": ""
    },
    {
        "id": "2507.20025",
        "title": "Region-based Cluster Discrimination for Visual Representation Learning",
        "translation": "Region-based Cluster Discrimination for Visual Representation Learning",
        "url": "https://arxiv.org/abs/2507.20025",
        "authors": "Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng",
        "publish_date": "2025-07-26T17:47:09.000Z",
        "summary": "Learning visual representations is foundational for a broad spectrum of\ndownstream tasks. Although recent vision-language contrastive models, such as\nCLIP and SigLIP, have achieved impressive zero-shot performance via large-scale\nvision-language alignment, their reliance on global representations constrains\ntheir effectiveness for dense prediction tasks, such as grounding, OCR, and\nsegmentation. To address this gap, we introduce Region-Aware Cluster\nDiscrimination (RICE), a novel method that enhances region-level visual and OCR\ncapabilities. We first construct a billion-scale candidate region dataset and\npropose a Region Transformer layer to extract rich regional semantics. We\nfurther design a unified region cluster discrimination loss that jointly\nsupports object and OCR learning within a single classification framework,\nenabling efficient and scalable distributed training on large-scale data.\nExtensive experiments show that RICE consistently outperforms previous methods\non tasks, including segmentation, dense detection, and visual perception for\nMultimodal Large Language Models (MLLMs). The pre-trained models have been\nreleased at https://github.com/deepglint/MVT.",
        "github_repo": "https://github.com/deepglint/MVT",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.19766",
        "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing   Large Language Models' Reasoning Abilities",
        "translation": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing   Large Language Models' Reasoning Abilities",
        "url": "https://arxiv.org/abs/2507.19766",
        "authors": "Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, Yang Li",
        "publish_date": "2025-07-26T03:42:33.000Z",
        "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.19058",
        "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with   Concept Relation Alignment",
        "translation": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with   Concept Relation Alignment",
        "url": "https://arxiv.org/abs/2507.19058",
        "authors": "Chong Xia, Shengjun Zhang, Fangfu Liu, Chang Liu, Khodchaphun Hirunyaratsameewong, Yueqi Duan",
        "publish_date": "2025-07-25T08:21:12.000Z",
        "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.",
        "github_repo": "https://github.com/xiac20/ScenePainter",
        "project_page": "https://xiac20.github.io/ScenePainter/",
        "model_function": ""
    },
    {
        "id": "2507.17189",
        "title": "Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for   Complex Meteorological Systems",
        "translation": "Met^2Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for   Complex Meteorological Systems",
        "url": "https://arxiv.org/abs/2507.17189",
        "authors": "Shaohan Li, Hao Yang, Min Chen, Xiaolin Qin",
        "publish_date": "2025-07-23T04:26:56.000Z",
        "summary": "The increasing frequency of extreme weather events due to global climate\nchange urges accurate weather prediction. Recently, great advances have been\nmade by the end-to-end methods, thanks to deep learning techniques,\nbut they face limitations of representation inconsistency in\nmultivariable integration and struggle to effectively capture the dependency\nbetween variables, which is required in complex weather systems. Treating\ndifferent variables as distinct modalities and applying a two-stage\ntraining approach from multimodal models can partially alleviate this issue,\nbut due to the inconformity in training tasks between the two stages, the\nresults are often suboptimal. To address these challenges, we propose an\nimplicit two-stage training method, configuring separate encoders and decoders\nfor each variable. In detailed, in the first stage, the Translator is frozen\nwhile the Encoders and Decoders learn a shared latent space, in the second\nstage, the Encoders and Decoders are frozen, and the Translator captures\ninter-variable interactions for prediction. Besides, by introducing a\nself-attention mechanism for multivariable fusion in the latent space, the\nperformance achieves further improvements. Empirically, extensive experiments\nshow the state-of-the-art performance of our method. Specifically, it reduces\nthe MSE for near-surface air temperature and relative humidity predictions by\n28.82\\% and 23.39\\%, respectively. The source code is available at\nhttps://github.com/ShremG/Met2Net.",
        "github_repo": "https://github.com/ShremG/Met2Net",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.19804",
        "title": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
        "translation": "ForCenNet: Foreground-Centric Network for Document Image Rectification",
        "url": "https://arxiv.org/abs/2507.19804",
        "authors": "Peng Cai, Qiang Li, Kaicheng Yang, Dong Guo, Jia Li, Nan Zhou, Xiang An, Ninghua Yang, Jiankang Deng",
        "publish_date": "2025-07-26T05:36:48.000Z",
        "summary": "Document image rectification aims to eliminate geometric deformation in\nphotographed documents to facilitate text recognition. However, existing\nmethods often neglect the significance of foreground elements, which provide\nessential geometric references and layout information for document image\ncorrection. In this paper, we introduce Foreground-Centric Network (ForCenNet)\nto eliminate geometric distortions in document images. Specifically, we\ninitially propose a foreground-centric label generation method, which extracts\ndetailed foreground elements from an undistorted image. Then we introduce a\nforeground-centric mask mechanism to enhance the distinction between readable\nand background regions. Furthermore, we design a curvature consistency loss to\nleverage the detailed foreground labels to help the model understand the\ndistorted geometric distribution. Extensive experiments demonstrate that\nForCenNet achieves new state-of-the-art on four real-world benchmarks, such as\nDocUNet, DIR300, WarpDoc, and DocReal. Quantitative analysis shows that the\nproposed method effectively undistorts layout elements, such as text lines and\ntable borders. The resources for further comparison are provided at\nhttps://github.com/caipeng328/ForCenNet.",
        "github_repo": "https://github.com/caipeng328/ForCenNet",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20880",
        "title": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment",
        "translation": "JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment",
        "url": "https://arxiv.org/abs/2507.20880",
        "authors": "Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria",
        "publish_date": "2025-07-28T14:34:02.000Z",
        "summary": "Diffusion and flow-matching models have revolutionized automatic\ntext-to-audio generation in recent times. These models are increasingly capable\nof generating high quality and faithful audio outputs capturing to speech and\nacoustic events. However, there is still much room for improvement in creative\naudio generation that primarily involves music and songs. Recent open\nlyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an\nacceptable standard in automatic song generation for recreational use. However,\nthese models lack fine-grained word-level controllability often desired by\nmusicians in their workflows. To the best of our knowledge, our\nflow-matching-based JAM is the first effort toward endowing word-level timing\nand duration control in song generation, allowing fine-grained vocal control.\nTo enhance the quality of generated songs to better align with human\npreferences, we implement aesthetic alignment through Direct Preference\nOptimization, which iteratively refines the model using a synthetic dataset,\neliminating the need or manual data annotations. Furthermore, we aim to\nstandardize the evaluation of such lyrics-to-song models through our public\nevaluation dataset JAME. We show that JAM outperforms the existing models in\nterms of the music-specific attributes.",
        "github_repo": "https://github.com/declare-lab/jamify",
        "project_page": "https://declare-lab.github.io/jamify",
        "model_function": ""
    },
    {
        "id": "2507.20900",
        "title": "Music Arena: Live Evaluation for Text-to-Music",
        "translation": "Music Arena: Live Evaluation for Text-to-Music",
        "url": "https://arxiv.org/abs/2507.20900",
        "authors": "Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue",
        "publish_date": "2025-07-28T14:52:57.000Z",
        "summary": "We present Music Arena, an open platform for scalable human preference\nevaluation of text-to-music (TTM) models. Soliciting human preferences via\nlistening studies is the gold standard for evaluation in TTM, but these studies\nare expensive to conduct and difficult to compare, as study protocols may\ndiffer across systems. Moreover, human preferences might help researchers align\ntheir TTM systems or improve automatic evaluation metrics, but an open and\nrenewable source of preferences does not currently exist. We aim to fill these\ngaps by offering *live* evaluation for TTM. In Music Arena, real-world users\ninput text prompts of their choosing and compare outputs from two TTM systems,\nand their preferences are used to compile a leaderboard. While Music Arena\nfollows recent evaluation trends in other AI domains, we also design it with\nkey features tailored to music: an LLM-based routing system to navigate the\nheterogeneous type signatures of TTM systems, and the collection of *detailed*\npreferences including listening data and natural language feedback. We also\npropose a rolling data release policy with user privacy guarantees, providing a\nrenewable source of preference data and increasing platform transparency.\nThrough its standardized evaluation protocol, transparent data access policies,\nand music-specific features, Music Arena not only addresses key challenges in\nthe TTM ecosystem but also demonstrates how live evaluation can be thoughtfully\nadapted to unique characteristics of specific AI domains.\n  Music Arena is available at: https://music-arena.org",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21848",
        "title": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for   Advantage Diversity",
        "translation": "EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for   Advantage Diversity",
        "url": "https://arxiv.org/abs/2507.21848",
        "authors": "Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang",
        "publish_date": "2025-07-29T14:23:58.000Z",
        "summary": "Large Language Models (LLMs) have made remarkable progress in enhancing\nstep-by-step reasoning through reinforcement learning. However, the Group\nRelative Policy Optimization (GRPO) algorithm, which relies on sparse reward\nrules, often encounters the issue of identical rewards within groups, leading\nto the advantage collapse problem. Existing works typically address this\nchallenge from two perspectives: enforcing model reflection to enhance response\ndiversity, and introducing internal feedback to augment the training signal\n(advantage). In this work, we begin by analyzing the limitations of model\nreflection and investigating the policy entropy of responses at the\nfine-grained sample level. Based on our experimental findings, we propose the\nEDGE-GRPO algorithm, which adopts Entropy-Driven Advantage\nand Guided Error Correction to effectively mitigate the\nproblem of advantage collapse. Extensive experiments on several main reasoning\nbenchmarks demonstrate the effectiveness and superiority of our approach. It is\navailable at https://github.com/ZhangXJ199/EDGE-GRPO.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.16806",
        "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
        "translation": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
        "url": "https://arxiv.org/abs/2507.16806",
        "authors": "Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas",
        "publish_date": "2025-07-22T17:56:01.000Z",
        "summary": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20527",
        "title": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful   Mathematics Questions and Answers",
        "translation": "SAND-Math: Using LLMs to Generate Novel, Difficult and Useful   Mathematics Questions and Answers",
        "url": "https://arxiv.org/abs/2507.20527",
        "authors": "Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, Emad Barsoum",
        "publish_date": "2025-07-28T05:17:48.000Z",
        "summary": "The demand for Large Language Models (LLMs) capable of sophisticated\nmathematical reasoning is growing across industries. However, the development\nof performant mathematical LLMs is critically bottlenecked by the scarcity of\ndifficult, novel training data. We introduce SAND-Math (Synthetic\nAugmented Novel and Difficult Mathematics problems and solutions), a pipeline\nthat addresses this by first generating high-quality problems from scratch and\nthen systematically elevating their complexity via a new Difficulty\nHiking step. We demonstrate the effectiveness of our approach through two key\nfindings. First, augmenting a strong baseline with SAND-Math data significantly\nboosts performance, outperforming the next-best synthetic dataset by\nuparrow 17.85 absolute points on the AIME25 benchmark. Second, in a\ndedicated ablation study, we show our Difficulty Hiking process is highly\neffective: by increasing average problem difficulty from 5.02 to 5.98, this\nstep lifts AIME25 performance from 46.38\\% to 49.23\\%. The full generation\npipeline, final dataset, and a fine-tuned model form a practical and scalable\ntoolkit for building more capable and efficient mathematical reasoning LLMs.\nSAND-Math dataset is released here:\nhttps://hf-mirror.com/datasets/amd/SAND-MATH{https://hf-mirror.com/datasets/amd/SAND-MATH}",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.20152",
        "title": "Goal Alignment in LLM-Based User Simulators for Conversational AI",
        "translation": "Goal Alignment in LLM-Based User Simulators for Conversational AI",
        "url": "https://arxiv.org/abs/2507.20152",
        "authors": "Shuhaib Mehri, Xiaocheng Yang, Takyoung Kim, Gokhan Tur, Shikib Mehri, Dilek Hakkani-TÃ¼r",
        "publish_date": "2025-07-27T07:07:12.000Z",
        "summary": "User simulators are essential to conversational AI, enabling scalable agent\ndevelopment and evaluation through simulated interactions. While current Large\nLanguage Models (LLMs) have advanced user simulation capabilities, we reveal\nthat they struggle to consistently demonstrate goal-oriented behavior across\nmulti-turn conversations--a critical limitation that compromises their\nreliability in downstream applications. We introduce User Goal State Tracking\n(UGST), a novel framework that tracks user goal progression throughout\nconversations. Leveraging UGST, we present a three-stage methodology for\ndeveloping user simulators that can autonomously track goal progression and\nreason to generate goal-aligned responses. Moreover, we establish comprehensive\nevaluation metrics for measuring goal alignment in user simulators, and\ndemonstrate that our approach yields substantial improvements across two\nbenchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a\ncritical gap in conversational AI and establish UGST as an essential framework\nfor developing goal-aligned user simulators.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.21035",
        "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via   Code-Driven Gene Expression Analysis",
        "translation": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via   Code-Driven Gene Expression Analysis",
        "url": "https://arxiv.org/abs/2507.21035",
        "authors": "Haoyang Liu, Yijiang Li, Haohan Wang",
        "publish_date": "2025-07-28T17:55:08.000Z",
        "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F_1 of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
        "github_repo": "https://github.com/Liu-Hy/GenoMAS",
        "project_page": "",
        "model_function": ""
    },
    {
        "id": "2507.19399",
        "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
        "translation": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security",
        "url": "https://arxiv.org/abs/2507.19399",
        "authors": "Gabriel Chua",
        "publish_date": "2025-07-25T16:06:16.000Z",
        "summary": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.",
        "github_repo": "",
        "project_page": "",
        "model_function": ""
    }
]