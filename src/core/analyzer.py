"""
AIåˆ†æå™¨æ¨¡å—
è´Ÿè´£ä½¿ç”¨AIåˆ†æè®ºæ–‡å†…å®¹å¹¶ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦
"""
import asyncio
import time
import threading
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Optional
from pathlib import Path

from ..utils.console import ConsoleOutput
from ..utils.logger import get_logger
from ..utils.file_utils import FileManager
from ..utils.progress import ProgressManager
from ..utils.ai_client import create_retryable_client
from ..models.paper import Paper
from ..models.report import AnalysisResult, DailyReport
from .parser import ContentParser
from .cache_manager import PaperCacheManager


class RageProgressTracker:
    """
    ğŸ”¥ ç‹‚æš´æ¨¡å¼å®æ—¶è¿›åº¦è·Ÿè¸ªå™¨
    """
    def __init__(self, total_papers: int, silent: bool = False):
        self.total = total_papers
        self.completed = 0
        self.success_count = 0
        self.fail_count = 0
        self.start_time = time.time()
        self.silent = silent
        self.lock = threading.Lock()
        self.stop_event = threading.Event()
        
        # å¯åŠ¨è¿›åº¦æ˜¾ç¤ºçº¿ç¨‹
        if not silent:
            self.progress_thread = threading.Thread(target=self._show_progress)
            self.progress_thread.daemon = True
            self.progress_thread.start()
    
    def update_progress(self, success: bool = True):
        """æ›´æ–°è¿›åº¦"""
        with self.lock:
            self.completed += 1
            if success:
                self.success_count += 1
            else:
                self.fail_count += 1
    
    def stop(self):
        """åœæ­¢è¿›åº¦æ˜¾ç¤º"""
        self.stop_event.set()
        if not self.silent and hasattr(self, 'progress_thread'):
            self.progress_thread.join(timeout=1)
            print()  # æ¢è¡Œï¼Œæ¸…é™¤è¿›åº¦æ¡
    
    def _show_progress(self):
        """å®æ—¶æ˜¾ç¤ºè¿›åº¦æ¡å’Œè®¡æ—¶ - å›ºå®šä½ç½®æ˜¾ç¤º"""
        import sys
        
        while not self.stop_event.is_set():
            with self.lock:
                # è®¡ç®—è¿›åº¦
                progress = self.completed / max(self.total, 1)
                percentage = progress * 100
                
                # åˆ›å»ºè¿›åº¦æ¡
                bar_width = 30
                filled = int(bar_width * progress)
                bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
                
                # è®¡ç®—è€—æ—¶
                elapsed = time.time() - self.start_time
                minutes, seconds = divmod(int(elapsed), 60)
                time_str = f"{minutes:02d}:{seconds:02d}"
                
                # æ˜¾ç¤ºè¿›åº¦æ¡
                sys.stdout.write(f'\rğŸ”¥ ç‹‚æš´æ¨¡å¼è¿›åº¦: [{bar}] {self.completed}/{self.total} ({percentage:.1f}%) | æˆåŠŸ:{self.success_count} å¤±è´¥:{self.fail_count} | è€—æ—¶:{time_str}')
                sys.stdout.flush()
            
            time.sleep(0.5)  # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡


class PaperAnalyzer:
    """
    è®ºæ–‡AIåˆ†æå™¨
    
    è´Ÿè´£ä½¿ç”¨AIåˆ†æè®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–çš„åˆ†æç»“æœ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.console = ConsoleOutput()
        self.logger = get_logger('analyzer')
        self.file_manager = FileManager('analyzer')
        self.parser = ContentParser()
        
        # è®¾ç½®é»˜è®¤é…ç½®
        self.output_dir = config.get('output_dir', 'data/daily_reports')
        self.ai_model = config.get('ai_model', 'zhipu')
        self.use_ai = config.get('use_ai', True)
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 2)
        
        # åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
        self.enable_cache = config.get('enable_cache', True)
        if self.enable_cache:
            cache_dir = str(Path(self.output_dir) / 'cache')
            self.cache_manager = PaperCacheManager(cache_dir)
        else:
            self.cache_manager = None
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.ai_client = None
        if self.use_ai:
            try:
                self.ai_client = create_retryable_client(
                    self.ai_model,
                    max_retries=self.max_retries
                )
            except Exception as e:
                self.logger.warning(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_ai = False
    
    def analyze_batch(self, papers: List[Paper], date: str = None, silent: bool = False) -> List[AnalysisResult]:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç”¨äºä¿å­˜ç»“æœï¼‰
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        if not papers:
            if not silent:
                self.console.print_warning("æ²¡æœ‰è®ºæ–‡éœ€è¦åˆ†æ")
            return []
        
        if not silent:
            self.console.print_header("AIåˆ†æç”Ÿæˆæ‘˜è¦", 3)
            self.console.print_info(f"å¼€å§‹é¡ºåºå¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(papers)} ç¯‡è®ºæ–‡")
        
        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        progress = ProgressManager(len(papers), "AIåˆ†æè®ºæ–‡") if not silent else None
        results = []
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†æ—¥æœŸï¼‰
        final_file = None
        if date:
            final_dir = Path(self.output_dir) / 'reports'
            self.file_manager.ensure_dir(final_dir)
            final_file = final_dir / f"{date}_report.json"
            
            # åŠ è½½å·²å­˜åœ¨çš„ç»“æœ
            existing_results = self._load_existing_results(final_file)
            existing_ids = {self._extract_paper_id_from_result(r) for r in existing_results}
        else:
            existing_ids = set()
        
        # ç»Ÿè®¡å˜é‡
        processed_count = 0
        success_count = 0
        fail_count = 0
        skip_count = 0

        # é¡ºåºå¤„ç†æ¯ç¯‡è®ºæ–‡
        for i, paper in enumerate(papers):
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            if date and paper.id in existing_ids:
                skip_count += 1
                if not silent:
                    self.console.print_skip(f"å·²å¤„ç†çš„è®ºæ–‡: {paper.id}")
                continue

            processed_count += 1

            if not silent:
                # æ˜¾ç¤ºå½“å‰å¤„ç†çš„è®ºæ–‡ä¿¡æ¯
                self.console.print_info(f"ğŸ” å¤„ç†ç¬¬ {i+1}/{len(papers)} é¡¹: {paper.translation}")

                # æ˜¾ç¤ºæ•´ä½“è¿›åº¦æ¡
                progress_bar = self._create_progress_bar(i, len(papers))
                remaining_papers = len(papers) - i - 1
                estimated_remaining = remaining_papers * 5  # è°ƒæ•´ä¸º5ç§’é¢„ä¼°ï¼ˆç®€åŒ–æç¤ºè¯ååº”è¯¥æ›´å¿«ï¼‰
                print(f"ğŸ“Š è¿›åº¦: {progress_bar} {i}/{len(papers)} (æˆåŠŸ:{success_count}, å¤±è´¥:{fail_count}, è·³è¿‡:{skip_count}) é¢„è®¡å‰©ä½™: {estimated_remaining}ç§’")
            
            self.logger.info(f"å¼€å§‹åˆ†æè®ºæ–‡: {paper.id} - {paper.title}")
            
            try:
                # åˆ†æå•ç¯‡è®ºæ–‡ï¼ˆä¿æŒä¸æ‰¹é‡åˆ†æç›¸åŒçš„é™é»˜çŠ¶æ€ï¼‰
                result = self.analyze_single(paper, silent=silent)
                
                if result:
                    # ç«‹å³ä¿å­˜ç»“æœï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶è·¯å¾„ï¼‰
                    if final_file:
                        self._save_single_result(result, final_file)

                    results.append(result)
                    success_count += 1

                    if progress:
                        progress.update(True, f"{paper.id}")

                    if not silent:
                        self.console.print_success(f"âœ… å®Œæˆ: {paper.id} ({i+1}/{len(papers)})")

                    # åªè®°å½•æ—¥å¿—ï¼Œä¸é‡å¤æ˜¾ç¤º
                    self.logger.info(f"è®ºæ–‡åˆ†æå®Œæˆ: {paper.id}")
                else:
                    fail_count += 1

                    if progress:
                        progress.update(False, f"{paper.id}")

                    if not silent:
                        self.console.print_error(f"âŒ å¤±è´¥: {paper.id} ({i+1}/{len(papers)})")

                    self.logger.error(f"è®ºæ–‡åˆ†æå¤±è´¥: {paper.id}")
                    
            except Exception as e:
                fail_count += 1

                if progress:
                    progress.update(False, f"{paper.id} - {e}")

                if not silent:
                    self.console.print_error(f"âŒ å¼‚å¸¸: {paper.id} - {e}")

                self.logger.error(f"è®ºæ–‡åˆ†æå¼‚å¸¸: {paper.id} - {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if progress:
            progress.finish()
        
        if not silent:
            # è®¡ç®—å®é™…å¤„ç†çš„è®ºæ–‡æ•°ï¼ˆæ’é™¤è·³è¿‡çš„ï¼‰
            actually_processed = processed_count

            self.console.print_summary("åˆ†æå®Œæˆç»Ÿè®¡", {
                "æ€»è®ºæ–‡æ•°": len(papers),
                "è·³è¿‡è®ºæ–‡": skip_count,
                "å®é™…å¤„ç†": actually_processed,
                "æˆåŠŸåˆ†æ": success_count,
                "åˆ†æå¤±è´¥": fail_count,
                "æˆåŠŸç‡": f"{success_count/max(actually_processed, 1)*100:.1f}%" if actually_processed > 0 else "0.0%"
            })

        self.logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼ŒæˆåŠŸ: {success_count}/{actually_processed}ï¼Œè·³è¿‡: {skip_count}")
        return results
    
    def analyze_batch_concurrent(self, papers: List[Paper], date: str = None, 
                                silent: bool = False, max_workers: int = 5) -> List[AnalysisResult]:
        """
        ğŸ”¥ ç‹‚æš´æ¨¡å¼ï¼šå¹¶å‘æ‰¹é‡åˆ†æè®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç”¨äºä¿å­˜ç»“æœï¼‰
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤5ï¼Œæ™ºè°±AIçš„å¹¶å‘é™åˆ¶ï¼‰
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        if not papers:
            if not silent:
                self.console.print_warning("æ²¡æœ‰è®ºæ–‡éœ€è¦åˆ†æ")
            return []
        
        if not silent:
            self.console.print_header("ğŸ”¥ ç‹‚æš´æ¨¡å¼ AIåˆ†æç”Ÿæˆæ‘˜è¦", 3)
            self.console.print_info(f"ğŸš€ å¯åŠ¨ {max_workers} å¹¶å‘å¤„ç† {len(papers)} ç¯‡è®ºæ–‡")
        
        self.logger.info(f"ğŸ”¥ ç‹‚æš´æ¨¡å¼ï¼šå¼€å§‹å¹¶å‘åˆ†æ {len(papers)} ç¯‡è®ºæ–‡ï¼Œå¹¶å‘æ•°: {max_workers}")
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†æ—¥æœŸï¼‰
        final_file = None
        existing_ids = set()
        if date:
            final_dir = Path(self.output_dir) / 'reports'
            self.file_manager.ensure_dir(final_dir)
            final_file = final_dir / f"{date}_report.json"
            
            # åŠ è½½å·²å­˜åœ¨çš„ç»“æœ
            existing_results = self._load_existing_results(final_file)
            existing_ids = {self._extract_paper_id_from_result(r) for r in existing_results}
        
        # è¿‡æ»¤å·²å¤„ç†çš„è®ºæ–‡
        papers_to_process = [p for p in papers if p.id not in existing_ids]
        skip_count = len(papers) - len(papers_to_process)
        
        if not papers_to_process:
            if not silent:
                self.console.print_info("æ‰€æœ‰è®ºæ–‡éƒ½å·²å¤„ç†ï¼Œè·³è¿‡åˆ†æ")
            return []
        
        if not silent and skip_count > 0:
            self.console.print_info(f"è·³è¿‡å·²å¤„ç†çš„ {skip_count} ç¯‡è®ºæ–‡")
        
        # åˆå§‹åŒ–å®æ—¶è¿›åº¦è·Ÿè¸ªå™¨
        progress_tracker = RageProgressTracker(len(papers_to_process), silent)
        
        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡è®¡æ•°å™¨
        import threading
        stats_lock = threading.Lock()
        stats = {
            'success_count': 0,
            'fail_count': 0,
            'processed_count': 0,
            'results': []
        }
        
        def analyze_single_threaded(paper):
            """çº¿ç¨‹å®‰å…¨çš„å•ç¯‡è®ºæ–‡åˆ†æ"""
            try:
                if not silent:
                    self.console.print_info(f"ğŸ” å¹¶å‘å¤„ç†: {paper.translation[:50]}...")
                
                # åˆ†æå•ç¯‡è®ºæ–‡ï¼ˆå†…éƒ¨é™é»˜æ¨¡å¼ï¼Œå‡å°‘æ—¥å¿—è¾“å‡ºï¼‰
                result = self.analyze_single(paper, silent=True)
                
                with stats_lock:
                    stats['processed_count'] += 1
                    # æ›´æ–°è¿›åº¦è·Ÿè¸ªå™¨
                    progress_tracker.update_progress(result is not None)
                    
                    if result:
                        stats['success_count'] += 1
                        stats['results'].append(result)
                        
                        # ç«‹å³ä¿å­˜ç»“æœï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶è·¯å¾„ï¼‰
                        if final_file:
                            self._save_single_result(result, final_file)
                        
                        if not silent:
                            current_processed = stats['processed_count']
                            total_to_process = len(papers_to_process)
                            self.console.print_success(f"âœ… å®Œæˆ: {paper.id} ({current_processed}/{total_to_process})")
                    else:
                        stats['fail_count'] += 1
                        if not silent:
                            self.console.print_error(f"âŒ å¤±è´¥: {paper.id}")
                
                return result
                
            except Exception as e:
                with stats_lock:
                    stats['processed_count'] += 1
                    stats['fail_count'] += 1
                
                if not silent:
                    self.console.print_error(f"âŒ å¼‚å¸¸: {paper.id} - {e}")
                
                self.logger.error(f"å¹¶å‘åˆ†æå¼‚å¸¸: {paper.id} - {e}")
                return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘åˆ†æ
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_paper = {
                executor.submit(analyze_single_threaded, paper): paper 
                for paper in papers_to_process
            }
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            from concurrent.futures import as_completed
            
            if not silent:
                self.console.print_info(f"âš¡ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†ä¸­...")
            
            for future in as_completed(future_to_paper):
                paper = future_to_paper[future]
                try:
                    future.result()  # è·å–ç»“æœï¼Œè§¦å‘å¼‚å¸¸å¤„ç†
                except Exception as e:
                    self.logger.error(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {paper.id} - {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # åœæ­¢è¿›åº¦è·Ÿè¸ªå™¨
        progress_tracker.stop()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if not silent:
            actually_processed = stats['processed_count']
            success_rate = f"{stats['success_count']/max(actually_processed, 1)*100:.1f}%" if actually_processed > 0 else "0.0%"
            avg_time_per_paper = total_time / max(actually_processed, 1)
            
            self.console.print_summary("ğŸ”¥ ç‹‚æš´æ¨¡å¼åˆ†æå®Œæˆç»Ÿè®¡", {
                "æ€»è®ºæ–‡æ•°": len(papers),
                "è·³è¿‡è®ºæ–‡": skip_count,
                "å¹¶å‘å¤„ç†": actually_processed,
                "æˆåŠŸåˆ†æ": stats['success_count'],
                "åˆ†æå¤±è´¥": stats['fail_count'],
                "æˆåŠŸç‡": success_rate,
                "æ€»è€—æ—¶": f"{total_time:.1f}ç§’",
                "å¹³å‡è€—æ—¶": f"{avg_time_per_paper:.1f}ç§’/ç¯‡",
                "å¹¶å‘æ•ˆç‡": f"{max_workers}x åŠ é€Ÿ"
            })

        self.logger.info(f"ğŸ”¥ ç‹‚æš´æ¨¡å¼åˆ†æå®Œæˆï¼ŒæˆåŠŸ: {stats['success_count']}/{stats['processed_count']}ï¼Œè·³è¿‡: {skip_count}ï¼Œè€—æ—¶: {total_time:.1f}ç§’")
        return stats['results']
    
    def analyze_single(self, paper: Paper, silent: bool = False) -> Optional[AnalysisResult]:
        """
        åˆ†æå•ç¯‡è®ºæ–‡ - æ”¯æŒç¼“å­˜æœºåˆ¶
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
            
        Returns:
            åˆ†æç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        # ğŸ¯ æ­¥éª¤1: æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and self.cache_manager:
            cached_result = self.cache_manager.get_cached_result(paper)
            if cached_result:
                if not silent:
                    self.console.print_info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜ç»“æœ: {paper.id}")
                return cached_result
        
        if not self.use_ai or not self.ai_client:
            if not silent:
                self.console.print_warning("AIåˆ†ææœªå¯ç”¨ï¼Œè¿”å›åŸºç¡€ç»“æœ")
            
            # è¿”å›åŸºç¡€ç»“æœ - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„
            # å¯¹äºåŸºç¡€ç»“æœï¼Œå¦‚æœæ²¡æœ‰AIç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡
            summary_zh = paper.summary[:200] + "..." if len(paper.summary) > 200 else paper.summary
            if not summary_zh or summary_zh == "æš‚æ— ":
                summary_zh = "æ— æ‘˜è¦ä¿¡æ¯"
            
            return AnalysisResult(
                id=paper.id,
                title_en=paper.title,
                title_zh=paper.translation,  # è¿™é‡Œä½¿ç”¨æ¸…æ´—æ—¶çš„translationå­—æ®µ
                url=paper.url,
                authors=paper.authors,
                publish_date=self._format_publish_date(paper.publish_date),
                summary_en=paper.summary,
                summary_zh=summary_zh,
                github_repo=paper.github_repo,
                project_page=paper.project_page,
                model_function="æš‚æ— "
            )
        
        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_delay = 2

        for attempt in range(max_retries):
            try:
                if not silent and attempt > 0:
                    self.console.print_info(f"é‡è¯•ç¬¬ {attempt} æ¬¡...")

                # æ„å»ºåˆ†ææç¤ºè¯
                prompt = self._build_analysis_prompt(paper)

                messages = [
                    {
                        "role": "user",
                        "content": [{
                            "type": "text",
                            "text": prompt
                        }]
                    }
                ]

                # è°ƒç”¨AIè¿›è¡Œåˆ†æï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
                import time
                import threading

                if not silent:
                    # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçº¿ç¨‹
                    progress_stop = threading.Event()
                    progress_thread = threading.Thread(
                        target=self._show_analysis_progress,
                        args=(progress_stop, f"åˆ†æè®ºæ–‡: {paper.translation[:30]}...")
                    )
                    progress_thread.daemon = True
                    progress_thread.start()

                start_time = time.time()

                try:
                    # ä½¿ç”¨çº¿ç¨‹è¶…æ—¶å¤„ç†ï¼ˆWindowså…¼å®¹ï¼‰
                    import concurrent.futures

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(self.ai_client.chat, messages)
                        try:
                            response = future.result(timeout=90)  # 90ç§’è¶…æ—¶
                        except concurrent.futures.TimeoutError:
                            if not silent:
                                progress_stop.set()
                                progress_thread.join(timeout=1)
                                print()  # æ¢è¡Œ
                            raise TimeoutError(f"AIè°ƒç”¨è¶…æ—¶ï¼ˆ90ç§’ï¼‰")

                except TimeoutError:
                    raise  # é‡æ–°æŠ›å‡ºè¶…æ—¶å¼‚å¸¸
                finally:
                    if not silent:
                        progress_stop.set()
                        progress_thread.join(timeout=1)
                        print()  # æ¢è¡Œ

                end_time = time.time()
                if not silent:
                    self.console.print_info(f"AIå“åº”è€—æ—¶: {end_time - start_time:.2f}ç§’")

                # å¦‚æœæˆåŠŸè·å¾—å“åº”ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                if response:
                    break
                else:
                    if attempt < max_retries - 1:
                        if not silent:
                            self.console.print_warning(f"AIå“åº”ä¸ºç©ºï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        self.logger.error(f"AIåˆ†æå¤±è´¥ï¼Œæ‰€æœ‰é‡è¯•éƒ½è¿”å›ç©ºå“åº”: {paper.id}")
                        return None

            except Exception as e:
                if attempt < max_retries - 1:
                    if not silent:
                        self.console.print_warning(f"AIè°ƒç”¨å¼‚å¸¸: {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    self.logger.warning(f"AIè°ƒç”¨å¼‚å¸¸ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                    time.sleep(retry_delay)
                    continue
                else:
                    self.logger.error(f"AIåˆ†æå¤±è´¥ï¼Œæ‰€æœ‰é‡è¯•éƒ½å¼‚å¸¸: {paper.id} - {e}")
                    return None

        # å¤„ç†AIå“åº”
        try:
            # è§£æAIå“åº”
            parsed_fields = self._parse_ai_response(response)

            # æ ¼å¼åŒ–å‘è¡¨æ—¥æœŸä¸ºYYYY-MM-DDæ ¼å¼
            publish_date = self._format_publish_date(paper.publish_date)

            # å¤„ç†ç¿»è¯‘å­—æ®µï¼Œç¡®ä¿ä¸ä¸ºç©º
            title_zh = parsed_fields.get('title_zh', '').strip()
            if not title_zh:
                title_zh = paper.title  # å¦‚æœAIæ²¡æœ‰æä¾›ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡
                self.logger.warning(f"ä½¿ç”¨è‹±æ–‡æ ‡é¢˜ä½œä¸ºä¸­æ–‡ç¿»è¯‘: {paper.id}")
            
            summary_zh = parsed_fields.get('summary_zh', '').strip()
            if not summary_zh:
                # å¦‚æœAIæ²¡æœ‰æä¾›æ‘˜è¦ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡æ‘˜è¦ï¼ˆæˆªå–å‰200å­—ç¬¦ï¼‰
                summary_zh = paper.summary[:200] + "..." if len(paper.summary) > 200 else paper.summary
                if not summary_zh or summary_zh == "æš‚æ— ":
                    summary_zh = "æ— æ‘˜è¦ä¿¡æ¯"
                self.logger.warning(f"ä½¿ç”¨è‹±æ–‡æ‘˜è¦ä½œä¸ºä¸­æ–‡ç¿»è¯‘: {paper.id}")

            # åˆ›å»ºåˆ†æç»“æœ - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„
            result = AnalysisResult(
                id=paper.id,
                title_en=paper.title,
                title_zh=title_zh,
                url=paper.url,
                authors=paper.authors,
                publish_date=publish_date,
                summary_en=paper.summary,
                summary_zh=summary_zh,
                github_repo=paper.github_repo,
                project_page=paper.project_page,
                model_function=parsed_fields.get('model_function', 'æš‚æ— ')
            )

            # ğŸ’¾ æ­¥éª¤2: ä¿å­˜åˆ°ç¼“å­˜
            if self.enable_cache and self.cache_manager:
                self.cache_manager.save_to_cache(paper, result)

            return result

        except Exception as e:
            self.logger.error(f"è§£æAIå“åº”å¼‚å¸¸: {paper.id} - {e}")
            return None
    
    def _build_analysis_prompt(self, paper: Paper) -> str:
        """
        æ„å»ºAIåˆ†ææç¤ºè¯
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            
        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        # ä»paperå¯¹è±¡ä¸­è·å–æ›´å¤šä¿¡æ¯
        authors = getattr(paper, 'authors', 'æš‚æ— ')
        summary = getattr(paper, 'summary', 'æš‚æ— ')
        github_repo = getattr(paper, 'github_repo', 'æš‚æ— ')
        project_page = getattr(paper, 'project_page', 'æš‚æ— ')
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªAIè®ºæ–‡ç¿»è¯‘å’Œåˆ†æä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„è®ºæ–‡ä¿¡æ¯è¿›è¡Œç¿»è¯‘å’Œåˆ†æï¼Œä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºç»“æœã€‚

## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
**æ ‡é¢˜ä¸­æ–‡ç¿»è¯‘**ï¼š[å¿…é¡»å°†è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆå‡†ç¡®çš„ä¸­æ–‡ï¼Œä¿æŒæŠ€æœ¯æœ¯è¯­çš„ä¸“ä¸šæ€§]
**æ‘˜è¦ä¸­æ–‡ç¿»è¯‘**ï¼š[å¿…é¡»å°†è‹±æ–‡æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œå³ä½¿æ‘˜è¦å¾ˆé•¿ä¹Ÿè¦å®Œæ•´ç¿»è¯‘]
**æ¨¡å‹åŠŸèƒ½**ï¼š[åŸºäºæ ‡é¢˜å’Œæ‘˜è¦åˆ†æçš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”ï¼Œ50å­—ä»¥å†…]

## é‡è¦æ³¨æ„äº‹é¡¹ï¼š
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œæ¯è¡Œä»¥å¯¹åº”æ ‡ç­¾å¼€å¤´
- æ¯ä¸ªå­—æ®µåé¢ç›´æ¥è·Ÿå…·ä½“å†…å®¹ï¼Œä¸è¦ä½¿ç”¨æ–¹æ‹¬å·
- æ ‡é¢˜ä¸­æ–‡ç¿»è¯‘å’Œæ‘˜è¦ä¸­æ–‡ç¿»è¯‘æ˜¯å¿…å¡«é¡¹ï¼Œç»å¯¹ä¸èƒ½å†™"æš‚æ— "æˆ–ç•™ç©º
- ç¿»è¯‘è¦å‡†ç¡®ä¸“ä¸šï¼Œä¿æŒæŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§
- æ¨¡å‹åŠŸèƒ½è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºæ ¸å¿ƒä»·å€¼
- å¦‚æœæ‘˜è¦è¿‡é•¿ï¼Œè¯·æå–æ ¸å¿ƒå†…å®¹è¿›è¡Œç¿»è¯‘ï¼Œä½†ä¸èƒ½çœç•¥

ã€å¾…ç¿»è¯‘å’Œåˆ†æçš„è®ºæ–‡ä¿¡æ¯ã€‘ï¼š
è®ºæ–‡IDï¼š{paper.id}
è‹±æ–‡æ ‡é¢˜ï¼š{paper.title}
ä½œè€…ï¼š{authors}
å‘è¡¨æ—¥æœŸï¼š{paper.publish_date}
è‹±æ–‡æ‘˜è¦ï¼š{summary if summary != 'æš‚æ— ' else 'æ— æ‘˜è¦ä¿¡æ¯'}
GitHubä»“åº“ï¼š{github_repo}
é¡¹ç›®é¡µé¢ï¼š{project_page}

è¯·åŠ¡å¿…å®Œæˆæ ‡é¢˜å’Œæ‘˜è¦çš„ä¸­æ–‡ç¿»è¯‘ï¼Œè¿™æ˜¯å¿…é¡»çš„ä»»åŠ¡ã€‚"""
        
        return prompt

    def _show_analysis_progress(self, stop_event, task_name):
        """
        æ˜¾ç¤ºAIåˆ†æè¿›åº¦åŠ¨ç”»

        Args:
            stop_event: åœæ­¢äº‹ä»¶
            task_name: ä»»åŠ¡åç§°
        """
        import sys
        import time

        spinner = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
        start_time = time.time()
        i = 0
        warning_shown = False

        while not stop_event.is_set():
            elapsed = int(time.time() - start_time)
            minutes, seconds = divmod(elapsed, 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # åœ¨75ç§’æ—¶æ˜¾ç¤ºè­¦å‘Š
            if elapsed >= 75 and not warning_shown:
                sys.stdout.write(f'\rğŸ§  {task_name} {spinner[i % len(spinner)]} å·²è€—æ—¶: {time_str} âš ï¸ å“åº”è¾ƒæ…¢...')
                warning_shown = True
            else:
                sys.stdout.write(f'\rğŸ§  {task_name} {spinner[i % len(spinner)]} å·²è€—æ—¶: {time_str}')

            sys.stdout.flush()

            time.sleep(0.1)
            i += 1

    def _create_progress_bar(self, current, total, width=50):
        """
        åˆ›å»ºè¿›åº¦æ¡

        Args:
            current: å½“å‰è¿›åº¦
            total: æ€»æ•°
            width: è¿›åº¦æ¡å®½åº¦

        Returns:
            è¿›åº¦æ¡å­—ç¬¦ä¸²
        """
        if total == 0:
            return "[" + "â–‘" * width + "]"

        progress = current / total
        filled = int(width * progress)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}]"
    
    def _parse_ai_response(self, response: str) -> Dict[str, str]:
        """
        è§£æAIå“åº”ï¼Œæå–ç¿»è¯‘å’Œåˆ†æç»“æœ
        
        Args:
            response: AIå“åº”æ–‡æœ¬
            
        Returns:
            è§£æåçš„å­—æ®µå­—å…¸
        """
        parsed_fields = {
            'title_zh': '',
            'summary_zh': '',
            'model_function': 'æš‚æ— '
        }
        
        try:
            lines = response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if line.startswith('**æ ‡é¢˜ä¸­æ–‡ç¿»è¯‘**ï¼š'):
                    parsed_fields['title_zh'] = line.replace('**æ ‡é¢˜ä¸­æ–‡ç¿»è¯‘**ï¼š', '').strip()
                elif line.startswith('**æ‘˜è¦ä¸­æ–‡ç¿»è¯‘**ï¼š'):
                    parsed_fields['summary_zh'] = line.replace('**æ‘˜è¦ä¸­æ–‡ç¿»è¯‘**ï¼š', '').strip()
                elif line.startswith('**æ¨¡å‹åŠŸèƒ½**ï¼š'):
                    parsed_fields['model_function'] = line.replace('**æ¨¡å‹åŠŸèƒ½**ï¼š', '').strip()
            
            # ç‰¹æ®Šå¤„ç†ï¼štitle_zh å’Œ summary_zh ä¸èƒ½ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨è‹±æ–‡åŸæ–‡
            if not parsed_fields['title_zh'] or parsed_fields['title_zh'].strip() == '':
                self.logger.warning("AIæœªæä¾›æ ‡é¢˜ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡")
                # è¿™é‡Œä¼šåœ¨è°ƒç”¨å¤„ä½¿ç”¨è‹±æ–‡æ ‡é¢˜ä½œä¸ºå¤‡é€‰
            
            if not parsed_fields['summary_zh'] or parsed_fields['summary_zh'].strip() == '':
                self.logger.warning("AIæœªæä¾›æ‘˜è¦ç¿»è¯‘ï¼Œä½¿ç”¨è‹±æ–‡åŸæ–‡")
                # è¿™é‡Œä¼šåœ¨è°ƒç”¨å¤„ä½¿ç”¨è‹±æ–‡æ‘˜è¦ä½œä¸ºå¤‡é€‰
            
            # model_function å¯ä»¥ä¸ºæš‚æ— 
            if not parsed_fields['model_function'] or parsed_fields['model_function'].strip() == '':
                parsed_fields['model_function'] = 'æš‚æ— '
                    
        except Exception as e:
            self.logger.warning(f"è§£æAIå“åº”å¤±è´¥: {e}")
        
        return parsed_fields
    
    def _format_publish_date(self, date_str: str) -> str:
        """
        æ ¼å¼åŒ–å‘è¡¨æ—¥æœŸä¸ºYYYY-MM-DDæ ¼å¼
        
        Args:
            date_str: åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            æ ¼å¼åŒ–åçš„æ—¥æœŸå­—ç¬¦ä¸²
        """
        if not date_str or date_str == 'æš‚æ— ':
            return 'æš‚æ— '
        
        try:
            # å¤„ç†ISOæ ¼å¼æ—¥æœŸ (2025-07-31T17:00:30.000Z)
            if 'T' in date_str:
                date_part = date_str.split('T')[0]
                return date_part
            
            # å¦‚æœå·²ç»æ˜¯YYYY-MM-DDæ ¼å¼
            if len(date_str) == 10 and date_str.count('-') == 2:
                return date_str
            
            # å…¶ä»–æ ¼å¼å°è¯•è§£æ
            import re
            from datetime import datetime
            
            # å°è¯•åŒ¹é…YYYY-MM-DDæ ¼å¼
            match = re.search(r'(\d{4}-\d{2}-\d{2})', date_str)
            if match:
                return match.group(1)
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
            return date_str
            
        except Exception as e:
            self.logger.warning(f"æ—¥æœŸæ ¼å¼åŒ–å¤±è´¥: {e}")
            return date_str
    
    def _load_existing_results(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        åŠ è½½å·²å­˜åœ¨çš„ç»“æœæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å·²å­˜åœ¨çš„ç»“æœåˆ—è¡¨
        """
        if file_path.exists():
            try:
                data = self.file_manager.load_json(file_path)
                return data if isinstance(data, list) else []
            except Exception as e:
                self.logger.error(f"åŠ è½½å·²å­˜åœ¨ç»“æœå¤±è´¥: {e}")
                return []
        return []
    
    def _save_single_result(self, result: AnalysisResult, file_path: Path):
        """
        ä¿å­˜å•ä¸ªåˆ†æç»“æœåˆ°æ–‡ä»¶
        
        Args:
            result: åˆ†æç»“æœ
            file_path: æ–‡ä»¶è·¯å¾„
        """
        try:
            # åŠ è½½ç°æœ‰ç»“æœ
            existing_results = self._load_existing_results(file_path)
            
            # ç§»é™¤å·²å­˜åœ¨çš„ç›¸åŒIDè®ºæ–‡
            existing_results = [
                r for r in existing_results 
                if self._extract_paper_id_from_result(r) != result.paper_id
            ]
            
            # æ·»åŠ æ–°ç»“æœ
            existing_results.append(result.to_dict())
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            self.file_manager.save_json(existing_results, file_path)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å•ä¸ªç»“æœå¤±è´¥: {e}")
    
    def _extract_paper_id_from_result(self, result: Dict[str, Any]) -> str:
        """
        ä»ç»“æœä¸­æå–è®ºæ–‡ID
        
        Args:
            result: ç»“æœå­—å…¸
            
        Returns:
            è®ºæ–‡ID
        """
        # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
        for field in ['paper_id', 'id']:
            if field in result:
                return result[field]
        
        # ä»URLä¸­æå–
        url = result.get('paper_url', '')
        if url:
            return url.split('/')[-1]
        
        return ''
    
    def create_daily_report(self, date: str, analysis_results: List[AnalysisResult]) -> DailyReport:
        """
        åˆ›å»ºæ—¥æŠ¥
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            æ—¥æŠ¥å¯¹è±¡
        """
        report = DailyReport(
            date=date,
            total_papers=len(analysis_results),
            analysis_results=analysis_results
        )
        
        return report
    
    def save_daily_report(self, report: DailyReport) -> bool:
        """
        ä¿å­˜æ—¥æŠ¥åˆ°æ–‡ä»¶
        
        Args:
            report: æ—¥æŠ¥å¯¹è±¡
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
            reports_dir = Path(self.output_dir) / 'reports'
            self.file_manager.ensure_dir(reports_dir)
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            file_path = reports_dir / f"{report.date}_report.json"
            
            # ä¿å­˜æŠ¥å‘Š
            success = report.save_to_file(str(file_path))
            
            if success:
                self.logger.info(f"æ—¥æŠ¥ä¿å­˜æˆåŠŸ: {file_path}")
            else:
                self.logger.error(f"æ—¥æŠ¥ä¿å­˜å¤±è´¥: {file_path}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ—¥æŠ¥å¼‚å¸¸: {e}")
            return False
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """
        è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        reports_dir = Path(self.output_dir) / 'reports'
        
        if not reports_dir.exists():
            return {"total_reports": 0, "reports": []}
        
        json_files = list(reports_dir.glob("*_report.json"))
        
        return {
            "total_reports": len(json_files),
            "reports": [f.stem for f in json_files],
            "reports_dir": str(reports_dir)
        }


# ä¾¿æ·å‡½æ•°
def create_analyzer(config: Dict[str, Any]) -> PaperAnalyzer:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºåˆ†æå™¨å®ä¾‹
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        PaperAnalyzerå®ä¾‹
    """
    return PaperAnalyzer(config)

def analyze_papers(papers: List[Paper], date: str = None,
                  output_dir: str = 'data/daily_reports',
                  ai_model: str = 'zhipu', silent: bool = False) -> List[AnalysisResult]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†æè®ºæ–‡
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        date: æ—¥æœŸå­—ç¬¦ä¸²
        output_dir: è¾“å‡ºç›®å½•
        ai_model: AIæ¨¡å‹ç±»å‹
        silent: æ˜¯å¦é™é»˜æ¨¡å¼
        
    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    config = {
        'output_dir': output_dir,
        'ai_model': ai_model
    }
    analyzer = PaperAnalyzer(config)
    return analyzer.analyze_batch(papers, date, silent)
