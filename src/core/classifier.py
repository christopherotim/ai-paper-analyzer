"""
æ™ºèƒ½åˆ†ç±»å™¨æ¨¡å—
è´Ÿè´£å¯¹è®ºæ–‡è¿›è¡Œæ™ºèƒ½åˆ†ç±»å¹¶ç”ŸæˆMDæ–‡ä»¶å’Œæ±‡æ€»æŠ¥å‘Š
"""
import os
import re
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from ..utils.console import ConsoleOutput
from ..utils.logger import get_logger
from ..utils.file_utils import FileManager
from ..utils.progress import ProgressManager
from ..utils.ai_client import create_retryable_client
from ..models.report import AnalysisResult, ClassificationResult, AnalysisSummary


class PaperClassifier:
    """
    è®ºæ–‡æ™ºèƒ½åˆ†ç±»å™¨
    
    è´Ÿè´£å¯¹åˆ†æç»“æœè¿›è¡Œæ™ºèƒ½åˆ†ç±»ï¼Œç”Ÿæˆåˆ†ç±»MDæ–‡ä»¶å’Œæ±‡æ€»æŠ¥å‘Š
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.console = ConsoleOutput()
        self.logger = get_logger('classifier')
        self.file_manager = FileManager('classifier')
        
        # è®¾ç½®é»˜è®¤é…ç½®
        self.output_dir = config.get('output_dir', 'data/analysis_results')
        self.ai_model = config.get('ai_model', 'zhipu')
        self.use_ai = config.get('use_ai', True)
        self.knowledge_file = config.get('knowledge_file', 'æ¨¡å‹åˆ†ç±».md')
        self.delay_between_requests = config.get('delay_between_requests', 1)
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.ai_client = None
        if self.use_ai:
            try:
                self.ai_client = create_retryable_client(
                    self.ai_model,
                    max_retries=3
                )
            except Exception as e:
                self.logger.warning(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_ai = False
        
        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = self._load_knowledge_base()
    
    def _load_knowledge_base(self) -> str:
        """
        åŠ è½½åˆ†ç±»çŸ¥è¯†åº“
        
        Returns:
            çŸ¥è¯†åº“å†…å®¹
        """
        if not os.path.exists(self.knowledge_file):
            self.logger.warning(f"çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.knowledge_file}")
            return ""
        
        try:
            with open(self.knowledge_file, 'r', encoding='utf-8') as f:
                content = f.read()
            self.logger.info(f"æˆåŠŸåŠ è½½çŸ¥è¯†åº“: {self.knowledge_file}")
            return content
        except Exception as e:
            self.logger.error(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
            return ""
    
    def split_to_md(self, analysis_results: List[AnalysisResult],
                   date: str, silent: bool = False) -> bool:
        """
        æ­¥éª¤1ï¼šåˆ‡åˆ†JSONä¸ºå•ä¸ªMDæ–‡ä»¶ï¼ˆç±»ä¼¼æ—§è„šæœ¬ï¼‰

        Args:
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²
            silent: æ˜¯å¦é™é»˜æ¨¡å¼

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not analysis_results:
            if not silent:
                self.console.print_warning("æ²¡æœ‰è®ºæ–‡éœ€è¦åˆ‡åˆ†")
            return False

        if not silent:
            self.console.print_info(f"å¼€å§‹åˆ‡åˆ† {len(analysis_results)} ç¯‡è®ºæ–‡ä¸ºMDæ–‡ä»¶")

        self.logger.info(f"å¼€å§‹MDåˆ‡åˆ† {len(analysis_results)} ç¯‡è®ºæ–‡")

        try:
            # åˆ›å»ºæ—¥æœŸç›®å½•
            date_dir = Path(self.output_dir) / date
            date_dir.mkdir(parents=True, exist_ok=True)

            # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºMDæ–‡ä»¶
            for i, analysis_result in enumerate(analysis_results):
                if not silent:
                    # æ˜¾ç¤ºå½“å‰å¤„ç†çš„è®ºæ–‡ä¿¡æ¯ï¼ˆç±»ä¼¼cleanerçš„ä½“éªŒï¼‰
                    self.console.print_info(f"ğŸ” åˆ‡åˆ†ç¬¬ {i+1}/{len(analysis_results)} ç¯‡: {analysis_result.title_zh[:50]}...")

                    # æ˜¾ç¤ºæ•´ä½“è¿›åº¦æ¡
                    progress_bar = self._create_progress_bar(i, len(analysis_results))
                    print(f"âœ‚ï¸ MDåˆ‡åˆ†è¿›åº¦: {progress_bar} {i}/{len(analysis_results)}")

                # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„
                safe_title = "".join(c for c in analysis_result.title_zh if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
                if not safe_title:
                    safe_title = f"paper_{analysis_result.id}"

                md_filename = f"{safe_title}.md"
                md_path = date_dir / md_filename

                # ç”ŸæˆMDå†…å®¹ - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„
                content = f"""# {analysis_result.title_zh}

**è®ºæ–‡ID**ï¼š{analysis_result.id}
**è‹±æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_en}
**ä¸­æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_zh}
**è®ºæ–‡åœ°å€**ï¼š{analysis_result.url}

**ä½œè€…å›¢é˜Ÿ**ï¼š{analysis_result.authors}
**å‘è¡¨æ—¥æœŸ**ï¼š{analysis_result.publish_date}

**è‹±æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_en}

**ä¸­æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_zh}

**GitHubä»“åº“**ï¼š{analysis_result.github_repo}
**é¡¹ç›®é¡µé¢**ï¼š{analysis_result.project_page}
**æ¨¡å‹åŠŸèƒ½**ï¼š{analysis_result.model_function}

**åˆ†ææ—¶é—´**ï¼š{analysis_result.analysis_time}
"""

                # å†™å…¥MDæ–‡ä»¶
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(content)

                if not silent:
                    self.console.print_success(f"âœ… åˆ‡åˆ†å®Œæˆ: {md_filename}")

                self.logger.info(f"MDæ–‡ä»¶åˆ›å»ºæˆåŠŸ: {md_path}")

                # æ·»åŠ å»¶è¿Ÿï¼ˆç±»ä¼¼cleanerçš„ä½“éªŒï¼‰
                if i < len(analysis_results) - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                    import time
                    time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿï¼Œè®©ç”¨æˆ·çœ‹åˆ°è¿›åº¦

            if not silent:
                self.console.print_success(f"ğŸ“ MDåˆ‡åˆ†å®Œæˆï¼Œè¾“å‡ºç›®å½•: {date_dir}")

            return True

        except Exception as e:
            if not silent:
                self.console.print_error(f"MDåˆ‡åˆ†å¤±è´¥: {e}")
            self.logger.error(f"MDåˆ‡åˆ†å¼‚å¸¸: {e}")
            return False

    def classify_papers(self, analysis_results: List[AnalysisResult],
                       date: str = None, silent: bool = False, rage_mode: bool = False) -> List[ClassificationResult]:
        """
        æ‰¹é‡åˆ†ç±»è®ºæ–‡

        Args:
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
            rage_mode: æ˜¯å¦å¯ç”¨ç‹‚æš´æ¨¡å¼ï¼ˆ5å¹¶å‘åˆ†ç±»ï¼‰

        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        if not analysis_results:
            if not silent:
                self.console.print_warning("æ²¡æœ‰è®ºæ–‡éœ€è¦åˆ†ç±»")
            return []

        if not silent:
            self.console.print_info(f"å¼€å§‹åˆ†ç±» {len(analysis_results)} ç¯‡è®ºæ–‡")
        
        self.logger.info(f"å¼€å§‹æ‰¹é‡åˆ†ç±» {len(analysis_results)} ç¯‡è®ºæ–‡")
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
        if rage_mode:
            if not silent:
                self.console.print_info("ğŸ”¥ ç‹‚æš´æ¨¡å¼ï¼šå¯åŠ¨5å¹¶å‘åˆ†ç±»...")
                self.console.print_warning("âš¡ æ³¨æ„ï¼šåˆ†ç±»AIè°ƒç”¨é¢‘ç‡è¾ƒé«˜ï¼Œè¯·ç¡®ä¿ç½‘ç»œç¨³å®š")
            return self._classify_papers_concurrent(analysis_results, date, silent)
        else:
            return self._classify_papers_sequential(analysis_results, date, silent)

    def _classify_papers_sequential(self, analysis_results: List[AnalysisResult], 
                                  date: str, silent: bool) -> List[ClassificationResult]:
        """ä¸²è¡Œåˆ†ç±»è®ºæ–‡"""
        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        progress = ProgressManager(len(analysis_results), "æ™ºèƒ½åˆ†ç±»è®ºæ–‡") if not silent else None
        results = []

        # ç»Ÿè®¡å˜é‡
        processed_count = 0
        success_count = 0
        fail_count = 0
        skip_count = 0
        
        # é¡ºåºå¤„ç†æ¯ç¯‡è®ºæ–‡
        for i, analysis_result in enumerate(analysis_results):
            if not silent:
                # æ˜¾ç¤ºå½“å‰å¤„ç†çš„è®ºæ–‡ä¿¡æ¯ï¼ˆç±»ä¼¼åŸºç¡€è„šæœ¬ï¼‰
                self.console.print_info(f"ğŸ” å¤„ç†ç¬¬ {i+1}/{len(analysis_results)} ç¯‡: {analysis_result.title_zh[:50]}...")

                # æ˜¾ç¤ºæ•´ä½“è¿›åº¦æ¡ï¼ˆç±»ä¼¼åŸºç¡€è„šæœ¬ï¼‰
                progress_bar = self._create_progress_bar(i, len(analysis_results))
                remaining_papers = len(analysis_results) - i - 1
                estimated_remaining = remaining_papers * 20  # å‡è®¾æ¯ç¯‡20ç§’
                print(f"ğŸ“Š è¿›åº¦: {progress_bar} {i}/{len(analysis_results)} (æˆåŠŸ:{success_count}, å¤±è´¥:{fail_count}, è·³è¿‡:{skip_count}) é¢„è®¡å‰©ä½™: {estimated_remaining}ç§’")

            self.logger.info(f"å¼€å§‹åˆ†ç±»è®ºæ–‡: {analysis_result.id}")

            try:
                # åˆ†ç±»å•ç¯‡è®ºæ–‡å¹¶ç«‹å³ä¿å­˜MDæ–‡ä»¶ï¼ˆç±»ä¼¼æ—§è„šæœ¬ï¼‰
                result = self.classify_and_save_single_paper(analysis_result, date, silent=silent)

                if result:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡çš„è®ºæ–‡ï¼ˆé€šè¿‡confidenceå’Œmd_contentåˆ¤æ–­ï¼‰
                    if result.confidence == 1.0 and result.md_content == "":
                        skip_count += 1
                    else:
                        results.append(result)
                        success_count += 1
                        processed_count += 1

                        if progress:
                            progress.update(True, f"{analysis_result.id}")

                        if not silent:
                            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼ˆç±»ä¼¼æ—§è„šæœ¬ï¼‰
                            self.console.print_success(f"âœ… åˆ†ç±»å®Œæˆ: {result.category} - {analysis_result.id}")

                        self.logger.info(f"è®ºæ–‡åˆ†ç±»å®Œæˆ: {analysis_result.id} -> {result.category}")
                else:
                    fail_count += 1
                    processed_count += 1

                    if progress:
                        progress.update(False, f"{analysis_result.id}")

                    if not silent:
                        self.console.print_error(f"âŒ åˆ†ç±»å¤±è´¥: {analysis_result.id}")

                    self.logger.error(f"è®ºæ–‡åˆ†ç±»å¤±è´¥: {analysis_result.id}")

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶ï¼ˆä¸æ—§è„šæœ¬ä¸€è‡´ï¼‰
                import time
                time.sleep(1)
                    
            except Exception as e:
                fail_count += 1
                processed_count += 1

                if progress:
                    progress.update(False, f"{analysis_result.id} - {e}")

                if not silent:
                    self.console.print_error(f"âŒ å¼‚å¸¸: {analysis_result.id} - {e}")

                self.logger.error(f"è®ºæ–‡åˆ†ç±»å¼‚å¸¸: {analysis_result.id} - {e}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if progress:
            progress.finish()

        if not silent:
            # è®¡ç®—å®é™…å¤„ç†çš„è®ºæ–‡æ•°ï¼ˆæ’é™¤è·³è¿‡çš„ï¼‰
            actually_processed = processed_count

            self.console.print_summary("åˆ†ç±»å®Œæˆç»Ÿè®¡", {
                "æ€»è®ºæ–‡æ•°": len(analysis_results),
                "è·³è¿‡è®ºæ–‡": skip_count,
                "å®é™…å¤„ç†": actually_processed,
                "æˆåŠŸåˆ†ç±»": success_count,
                "åˆ†ç±»å¤±è´¥": fail_count,
                "æˆåŠŸç‡": f"{success_count/max(actually_processed, 1)*100:.1f}%" if actually_processed > 0 else "0.0%"
            })

        self.logger.info(f"æ‰¹é‡åˆ†ç±»å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{actually_processed}ï¼Œè·³è¿‡: {skip_count}")
        return results

    def _classify_papers_concurrent(self, analysis_results: List[AnalysisResult], 
                                  date: str, silent: bool) -> List[ClassificationResult]:
        """ğŸ”¥ ç‹‚æš´æ¨¡å¼ï¼šå¹¶å‘åˆ†ç±»è®ºæ–‡"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import threading
        import time
        
        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡è®¡æ•°å™¨
        stats_lock = threading.Lock()
        stats = {
            'success_count': 0,
            'fail_count': 0,
            'skip_count': 0,
            'processed_count': 0,
            'results': []
        }
        
        def classify_single_threaded(analysis_result):
            """çº¿ç¨‹å®‰å…¨çš„å•ç¯‡è®ºæ–‡åˆ†ç±»"""
            try:
                if not silent:
                    self.console.print_info(f"ğŸ” å¹¶å‘åˆ†ç±»: {analysis_result.title_zh[:50]}...")
                
                # åˆ†ç±»å•ç¯‡è®ºæ–‡å¹¶ç«‹å³ä¿å­˜MDæ–‡ä»¶ï¼ˆå†…éƒ¨é™é»˜æ¨¡å¼ï¼‰
                result = self.classify_and_save_single_paper(analysis_result, date, silent=True)
                
                with stats_lock:
                    stats['processed_count'] += 1
                    if result:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼“å­˜å‘½ä¸­
                        if result.md_content == "CACHED":
                            stats['skip_count'] += 1
                            if not silent:
                                current_processed = stats['processed_count']
                                total_to_process = len(analysis_results)
                                self.console.print_skip(f"â­ï¸ è·³è¿‡å·²åˆ†ç±»: {result.category} - {analysis_result.id} ({current_processed}/{total_to_process})")
                        else:
                            stats['success_count'] += 1
                            stats['results'].append(result)
                            if not silent:
                                current_processed = stats['processed_count']
                                total_to_process = len(analysis_results)
                                self.console.print_success(f"âœ… åˆ†ç±»å®Œæˆ: {result.category} - {analysis_result.id} ({current_processed}/{total_to_process})")
                    else:
                        stats['fail_count'] += 1
                        if not silent:
                            self.console.print_error(f"âŒ åˆ†ç±»å¤±è´¥: {analysis_result.id}")
                
                return result
                
            except Exception as e:
                with stats_lock:
                    stats['processed_count'] += 1
                    stats['fail_count'] += 1
                if not silent:
                    self.console.print_error(f"âŒ å¼‚å¸¸: {analysis_result.id} - {e}")
                self.logger.error(f"å¹¶å‘åˆ†ç±»å¼‚å¸¸: {analysis_result.id} - {e}")
                return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘åˆ†ç±»
        start_time = time.time()
        max_workers = 5  # æ™ºè°±AIçš„å¹¶å‘é™åˆ¶
        
        # åˆ›å»ºè¿›åº¦æ¡æ˜¾ç¤ºçº¿ç¨‹
        progress_stop = threading.Event()
        if not silent:
            progress_thread = threading.Thread(
                target=self._show_rage_mode_progress,
                args=(progress_stop, stats, stats_lock, len(analysis_results), start_time)
            )
            progress_thread.daemon = True
            progress_thread.start()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_result = {
                executor.submit(classify_single_threaded, analysis_result): analysis_result 
                for analysis_result in analysis_results
            }
            
            if not silent:
                self.console.print_info(f"âš¡ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘åˆ†ç±»ä¸­...")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_result):
                analysis_result = future_to_result[future]
                try:
                    future.result()  # è·å–ç»“æœï¼Œè§¦å‘å¼‚å¸¸å¤„ç†
                except Exception as e:
                    self.logger.error(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {analysis_result.id} - {e}")
        
        # åœæ­¢è¿›åº¦æ¡æ˜¾ç¤º
        if not silent:
            progress_stop.set()
            progress_thread.join(timeout=1)
            print()  # æ¢è¡Œ
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if not silent:
            actually_processed = stats['processed_count'] - stats['skip_count']
            success_rate = f"{stats['success_count']/max(actually_processed, 1)*100:.1f}%" if actually_processed > 0 else "0.0%"
            avg_time_per_paper = total_time / max(actually_processed, 1)
            
            self.console.print_summary("ğŸ”¥ ç‹‚æš´æ¨¡å¼åˆ†ç±»å®Œæˆç»Ÿè®¡", {
                "æ€»è®ºæ–‡æ•°": len(analysis_results),
                "è·³è¿‡è®ºæ–‡": stats['skip_count'],
                "å¹¶å‘å¤„ç†": actually_processed,
                "æˆåŠŸåˆ†ç±»": stats['success_count'],
                "åˆ†ç±»å¤±è´¥": stats['fail_count'],
                "æˆåŠŸç‡": success_rate,
                "æ€»è€—æ—¶": f"{total_time:.1f}ç§’",
                "å¹³å‡è€—æ—¶": f"{avg_time_per_paper:.1f}ç§’/ç¯‡",
                "å¹¶å‘æ•ˆç‡": f"{max_workers}x åŠ é€Ÿ"
            })
        
        self.logger.info(f"ğŸ”¥ ç‹‚æš´æ¨¡å¼åˆ†ç±»å®Œæˆï¼ŒæˆåŠŸ: {stats['success_count']}/{stats['processed_count']}ï¼Œè·³è¿‡: {stats['skip_count']}ï¼Œè€—æ—¶: {total_time:.1f}ç§’")
        return stats['results']
    
    def classify_single_paper(self, analysis_result: AnalysisResult, 
                             silent: bool = False) -> Optional[ClassificationResult]:
        """
        åˆ†ç±»å•ç¯‡è®ºæ–‡
        
        Args:
            analysis_result: åˆ†æç»“æœ
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
            
        Returns:
            åˆ†ç±»ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        if not self.use_ai or not self.ai_client:
            if not silent:
                self.console.print_warning("AIåˆ†ç±»æœªå¯ç”¨ï¼Œè¿”å›é»˜è®¤åˆ†ç±»")
            
            # è¿”å›é»˜è®¤åˆ†ç±»ç»“æœ
            return ClassificationResult(
                paper_id=analysis_result.id,
                category="å¤šæ¨¡æ€ç”Ÿæˆ",
                confidence=0.5,
                md_content=self._generate_default_md_content(analysis_result)
            )
        
        try:
            # æ„å»ºåˆ†ç±»æç¤ºè¯
            prompt = self._build_classification_prompt(analysis_result)

            messages = [
                {
                    "role": "user",
                    "content": [{
                        "type": "text",
                        "text": prompt
                    }]
                }
            ]

            # AIè°ƒç”¨ï¼ˆå¸¦å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼‰
            import threading
            import time
            
            if not silent:
                # åˆ›å»ºè¿›åº¦æ˜¾ç¤ºçº¿ç¨‹
                progress_stop = threading.Event()
                progress_thread = threading.Thread(
                    target=self._show_classification_progress,
                    args=(progress_stop, f"åˆ†ç±»è®ºæ–‡: {analysis_result.title_zh[:30]}...")
                )
                progress_thread.daemon = True
                progress_thread.start()

            start_time = time.time()

            try:
                # ç›´æ¥è°ƒç”¨AI
                response = self.ai_client.chat(messages)
            finally:
                if not silent:
                    progress_stop.set()
                    progress_thread.join(timeout=1)
                    print()  # æ¢è¡Œ

            if not silent:
                end_time = time.time()
                self.console.print_info(f"AIå“åº”è€—æ—¶: {end_time - start_time:.2f}ç§’")

            if not response:
                self.logger.error(f"AIåˆ†ç±»å¤±è´¥ï¼Œå“åº”ä¸ºç©º: {analysis_result.id}")
                return None

        except Exception as e:
            self.logger.error(f"AIåˆ†ç±»å¼‚å¸¸: {analysis_result.id} - {e}")
            return None

        # å¤„ç†AIå“åº”
        try:
            
            if response:
                # è§£æAIå“åº”
                category, confidence, md_content = self._parse_classification_response(response)
                
                # åˆ›å»ºåˆ†ç±»ç»“æœ
                result = ClassificationResult(
                    paper_id=analysis_result.id,
                    category=category,
                    confidence=confidence,
                    md_content=md_content
                )
                
                return result
            else:
                self.logger.error(f"AIåˆ†ç±»è¿”å›ç©ºç»“æœ: {analysis_result.id}")
                return None
                
        except Exception as e:
            self.logger.error(f"åˆ†ç±»è®ºæ–‡å¼‚å¸¸: {analysis_result.id} - {e}")
            return None

    def classify_and_save_single_paper(self, analysis_result: AnalysisResult,
                                     date: str, silent: bool = False) -> Optional[ClassificationResult]:
        """
        åˆ†ç±»å•ç¯‡è®ºæ–‡å¹¶ç«‹å³ä¿å­˜MDæ–‡ä»¶ï¼ˆç±»ä¼¼æ—§è„šæœ¬ï¼‰

        Args:
            analysis_result: åˆ†æç»“æœ
            date: æ—¥æœŸå­—ç¬¦ä¸²
            silent: æ˜¯å¦é™é»˜æ¨¡å¼

        Returns:
            åˆ†ç±»ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        # ç”ŸæˆåŸå§‹MDæ–‡ä»¶åï¼ˆä¸æ­¥éª¤1åˆ‡åˆ†æ—¶ä¸€è‡´ï¼‰
        safe_title = "".join(c for c in analysis_result.title_zh if c.isalnum() or c in (' ', '-', '_')).rstrip()
        safe_title = safe_title[:50]  # é™åˆ¶é•¿åº¦
        if not safe_title:
            safe_title = f"paper_{analysis_result.id}"

        original_md_filename = f"{safe_title}.md"

        # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨åˆ†ç±»æ–‡ä»¶
        date_dir = Path(self.output_dir) / date
        if date_dir.exists():
            for category_dir in date_dir.iterdir():
                if category_dir.is_dir():
                    expected_file = category_dir / original_md_filename
                    if expected_file.exists():
                        if not silent:
                            self.console.print_skip(f"å·²å¤„ç†çš„è®ºæ–‡: {analysis_result.paper_id}")

                        # è¿”å›å·²å­˜åœ¨çš„åˆ†ç±»ç»“æœï¼Œæ ‡è®°ä¸ºç¼“å­˜å‘½ä¸­
                        return ClassificationResult(
                            paper_id=analysis_result.id,
                            category=category_dir.name,
                            confidence=1.0,
                            md_content="CACHED"  # æ ‡è®°ä¸ºç¼“å­˜å‘½ä¸­
                        )

        # æ‰§è¡Œåˆ†ç±»
        result = self.classify_single_paper(analysis_result, silent)

        if result:
            # ç«‹å³ä¿å­˜MDæ–‡ä»¶åˆ°åˆ†ç±»ç›®å½•ï¼ˆç±»ä¼¼æ—§è„šæœ¬ï¼‰
            try:
                # åˆ›å»ºåˆ†ç±»ç›®å½•
                category_dir = date_dir / result.category
                category_dir.mkdir(parents=True, exist_ok=True)

                # ä½¿ç”¨åŸå§‹MDæ–‡ä»¶åï¼ˆä¸æ—§è„šæœ¬ä¸€è‡´ï¼‰
                md_filename = original_md_filename
                md_path = category_dir / md_filename

                # å†™å…¥åˆ†ç±»åçš„MDæ–‡ä»¶
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(result.md_content)

                if not silent:
                    self.console.print_success(f"âœ… åˆ†ç±»å®Œæˆ: {result.category} - {md_filename}")

                self.logger.info(f"MDæ–‡ä»¶ä¿å­˜æˆåŠŸ: {md_path}")

            except Exception as e:
                if not silent:
                    self.console.print_error(f"MDæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
                self.logger.error(f"MDæ–‡ä»¶ä¿å­˜å¼‚å¸¸: {analysis_result.paper_id} - {e}")

        return result

    def _build_classification_prompt(self, analysis_result: AnalysisResult) -> str:
        """
        æ„å»ºåˆ†ç±»æç¤ºè¯ - åœ¨åŸæœ‰MDåŸºç¡€ä¸Šå¢åŠ æŠ€æœ¯ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            æç¤ºè¯å­—ç¬¦ä¸²
        """
        # ç”ŸæˆåŸºç¡€MDå†…å®¹
        base_md_content = f"""# {analysis_result.title_zh}

**è®ºæ–‡ID**ï¼š{analysis_result.id}
**è‹±æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_en}
**ä¸­æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_zh}
**è®ºæ–‡åœ°å€**ï¼š{analysis_result.url}

**ä½œè€…å›¢é˜Ÿ**ï¼š{analysis_result.authors}
**å‘è¡¨æ—¥æœŸ**ï¼š{analysis_result.publish_date}

**è‹±æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_en}

**ä¸­æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_zh}

**GitHubä»“åº“**ï¼š{analysis_result.github_repo}
**é¡¹ç›®é¡µé¢**ï¼š{analysis_result.project_page}
**æ¨¡å‹åŠŸèƒ½**ï¼š{analysis_result.model_function}

**åˆ†ææ—¶é—´**ï¼š{analysis_result.analysis_time}
"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªAIæ¨¡å‹åˆ†ç±»ä¸æ€»ç»“ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„è®ºæ–‡ä¿¡æ¯è¿›è¡Œåˆ†ç±»å’Œæ€»ç»“ï¼Œåœ¨åŸæœ‰MDå†…å®¹åŸºç¡€ä¸Šå¢åŠ "æŠ€æœ¯ç‰¹ç‚¹"å’Œ"åº”ç”¨åœºæ™¯"ä¸¤ä¸ªå­—æ®µã€‚

## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
# [åˆ†ç±»åç§°]

# [ä¸­æ–‡æ ‡é¢˜]

**è®ºæ–‡ID**ï¼š{analysis_result.id}
**è‹±æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_en}
**ä¸­æ–‡æ ‡é¢˜**ï¼š{analysis_result.title_zh}
**è®ºæ–‡åœ°å€**ï¼š{analysis_result.url}

**ä½œè€…å›¢é˜Ÿ**ï¼š{analysis_result.authors}
**å‘è¡¨æ—¥æœŸ**ï¼š{analysis_result.publish_date}

**è‹±æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_en}

**ä¸­æ–‡æ‘˜è¦**ï¼š
{analysis_result.summary_zh}

**GitHubä»“åº“**ï¼š{analysis_result.github_repo}
**é¡¹ç›®é¡µé¢**ï¼š{analysis_result.project_page}
**æ¨¡å‹åŠŸèƒ½**ï¼š{analysis_result.model_function}

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š[åŸºäºè®ºæ–‡å†…å®¹æ€»ç»“çš„ä¸»è¦æŠ€æœ¯åˆ›æ–°ç‚¹ï¼Œ2-3å¥è¯ï¼Œçªå‡ºä¸ç°æœ‰æ–¹æ³•çš„åŒºåˆ«]

**åº”ç”¨åœºæ™¯**ï¼š[åŸºäºè®ºæ–‡å†…å®¹åˆ—ä¸¾çš„2-3ä¸ªå…·ä½“åº”ç”¨åœºæ™¯ï¼Œè¦å…·ä½“å¯è¡Œ]

**åˆ†ææ—¶é—´**ï¼š{analysis_result.analysis_time}

## åˆ†ç±»è§„åˆ™ï¼š
- å¿…é¡»ä»ä»¥ä¸‹åˆ†ç±»ä¸­é€‰æ‹©ï¼šæ–‡æœ¬ç”Ÿæˆã€éŸ³é¢‘ç”Ÿæˆã€å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆã€å¤šæ¨¡æ€ç”Ÿæˆã€3Dç”Ÿæˆã€æ¸¸æˆä¸ç­–ç•¥ç”Ÿæˆã€ç§‘å­¦è®¡ç®—ä¸æ•°æ®ç”Ÿæˆã€ä»£ç ç”Ÿæˆä¸æ•°æ®å¢å¼ºã€è·¨æ¨¡æ€ç”Ÿæˆ
- å¦‚æœä¸ç¡®å®šï¼Œé€‰æ‹©"å¤šæ¨¡æ€ç”Ÿæˆ"
- å¦‚æœæ¨¡å‹æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œé€‰æ‹©æœ€ä¸»è¦çš„åŠŸèƒ½åˆ†ç±»

## é‡è¦æ³¨æ„äº‹é¡¹ï¼š
- åˆ†ç±»åç§°å¿…é¡»å®Œå…¨åŒ¹é…ä¸Šè¿°åˆ†ç±»åˆ—è¡¨
- ä¿æŒåŸæœ‰MDå†…å®¹çš„å®Œæ•´æ€§ï¼Œåªå¢åŠ "æŠ€æœ¯ç‰¹ç‚¹"å’Œ"åº”ç”¨åœºæ™¯"ä¸¤ä¸ªå­—æ®µ
- æŠ€æœ¯ç‰¹ç‚¹è¦åŸºäºè®ºæ–‡çš„å®é™…æŠ€æœ¯åˆ›æ–°ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
- åº”ç”¨åœºæ™¯è¦å…·ä½“ï¼Œé¿å…"å¤šç§åº”ç”¨åœºæ™¯"è¿™æ ·çš„æ¨¡ç³Šæè¿°
- æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»å¡«å†™å®Œæ•´ï¼Œä¸èƒ½ç•™ç©º

## æ¨¡å‹åˆ†ç±»çŸ¥è¯†åº“ï¼š
{self.knowledge_base}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯è¿›è¡Œåˆ†ç±»å’Œæ€»ç»“ã€‚"""
        
        return prompt

    def _create_progress_bar(self, current, total, width=50):
        """
        åˆ›å»ºè¿›åº¦æ¡ï¼ˆç±»ä¼¼æ—§è„šæœ¬çš„tqdmé£æ ¼ï¼‰

        Args:
            current: å½“å‰è¿›åº¦
            total: æ€»æ•°
            width: è¿›åº¦æ¡å®½åº¦

        Returns:
            è¿›åº¦æ¡å­—ç¬¦ä¸²
        """
        if total == 0:
            return "[" + "â–‘" * width + "]"

        progress = current / total
        filled = int(width * progress)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}]"

    def _show_classification_progress(self, stop_event, task_name):
        """
        æ˜¾ç¤ºAIåˆ†ç±»è¿›åº¦åŠ¨ç”»ï¼ˆå®æ—¶é¢„è§ˆæ—¶é—´ï¼‰

        Args:
            stop_event: åœæ­¢äº‹ä»¶
            task_name: ä»»åŠ¡åç§°
        """
        import sys
        import time

        spinner = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']
        start_time = time.time()
        i = 0
        warning_shown = False

        while not stop_event.is_set():
            elapsed = int(time.time() - start_time)
            minutes, seconds = divmod(elapsed, 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # åœ¨60ç§’æ—¶æ˜¾ç¤ºè­¦å‘Š
            if elapsed >= 60 and not warning_shown:
                sys.stdout.write(f'\rğŸ·ï¸ {task_name} {spinner[i % len(spinner)]} å·²è€—æ—¶: {time_str} âš ï¸ å“åº”è¾ƒæ…¢...')
                warning_shown = True
            else:
                sys.stdout.write(f'\rğŸ·ï¸ {task_name} {spinner[i % len(spinner)]} å·²è€—æ—¶: {time_str}')

            sys.stdout.flush()

            time.sleep(0.1)
            i += 1

    def _show_rage_mode_progress(self, stop_event, stats, stats_lock, total_papers, start_time):
        """
        æ˜¾ç¤ºç‹‚æš´æ¨¡å¼å®æ—¶è¿›åº¦æ¡å’Œè®¡æ—¶
        
        Args:
            stop_event: åœæ­¢äº‹ä»¶
            stats: ç»Ÿè®¡æ•°æ®å­—å…¸
            stats_lock: ç»Ÿè®¡æ•°æ®é”
            total_papers: æ€»è®ºæ–‡æ•°
            start_time: å¼€å§‹æ—¶é—´
        """
        import sys
        import time
        
        while not stop_event.is_set():
            with stats_lock:
                processed = stats['processed_count']
                success = stats['success_count']
                skip = stats['skip_count']
                fail = stats['fail_count']
            
            # è®¡ç®—è¿›åº¦
            progress = processed / max(total_papers, 1)
            percentage = progress * 100
            
            # åˆ›å»ºè¿›åº¦æ¡
            bar_width = 30
            filled = int(bar_width * progress)
            bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
            
            # è®¡ç®—è€—æ—¶
            elapsed = time.time() - start_time
            minutes, seconds = divmod(int(elapsed), 60)
            time_str = f"{minutes:02d}:{seconds:02d}"
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            sys.stdout.write(f'\rğŸ”¥ ç‹‚æš´æ¨¡å¼è¿›åº¦: [{bar}] {processed}/{total_papers} ({percentage:.1f}%) | æˆåŠŸ:{success} è·³è¿‡:{skip} å¤±è´¥:{fail} | è€—æ—¶:{time_str}')
            sys.stdout.flush()
            
            time.sleep(0.5)  # æ¯0.5ç§’æ›´æ–°ä¸€æ¬¡



    def generate_summary_report(self, date: str, silent: bool = False) -> bool:
        """
        ç”Ÿæˆåˆ†ç±»æ±‡æ€»æŠ¥å‘Šï¼ˆç±»ä¼¼æ—§è„šæœ¬åŠŸèƒ½ï¼‰

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²
            silent: æ˜¯å¦é™é»˜æ¨¡å¼

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            output_dir = Path(self.output_dir) / date

            if not output_dir.exists():
                if not silent:
                    self.console.print_warning(f"åˆ†ç±»ç›®å½•ä¸å­˜åœ¨: {output_dir}")
                return False

            # ç»Ÿè®¡å„åˆ†ç±»çš„è®ºæ–‡æ•°é‡
            categories = {}

            for item in output_dir.iterdir():
                if item.is_dir() and not item.name.startswith('.'):
                    # ç»Ÿè®¡è¯¥åˆ†ç±»ä¸‹çš„MDæ–‡ä»¶æ•°é‡
                    md_count = len([f for f in item.iterdir() if f.suffix == '.md'])
                    if md_count > 0:
                        categories[item.name] = md_count

            if not categories:
                if not silent:
                    self.console.print_warning("æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»ç»“æœ")
                return False

            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå†…å®¹
            total_papers = sum(categories.values())

            summary_content = f"# è®ºæ–‡åˆ†ç±»æ±‡æ€»æŠ¥å‘Š\n\n"
            summary_content += f"ç”Ÿæˆæ—¶é—´ï¼š{date}\n\n"
            summary_content += f"## åˆ†ç±»ç»Ÿè®¡\n\n"
            summary_content += f"- **æ€»è®ºæ–‡æ•°**ï¼š{total_papers} ç¯‡\n"
            summary_content += f"- **åˆ†ç±»æ•°é‡**ï¼š{len(categories)} ä¸ª\n\n"
            summary_content += f"## å„åˆ†ç±»è¯¦æƒ…\n\n"

            # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åˆ—
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                percentage = count / total_papers * 100
                summary_content += f"### {category}\n"
                summary_content += f"- è®ºæ–‡æ•°é‡ï¼š{count} ç¯‡\n"
                summary_content += f"- å æ¯”ï¼š{percentage:.1f}%\n\n"

            # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
            summary_file = output_dir / "æ¨¡å‹åˆ†ç±»æ±‡æ€».md"

            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(summary_content)

            if not silent:
                self.console.print_info(f"ğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
                self.console.print_info(f"   - æ€»è®ºæ–‡æ•°: {total_papers} ç¯‡")
                self.console.print_info(f"   - åˆ†ç±»æ•°é‡: {len(categories)} ä¸ª")
                for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                    percentage = count / total_papers * 100
                    self.console.print_info(f"   - {category}: {count} ç¯‡ ({percentage:.1f}%)")

            self.logger.info(f"æ±‡æ€»æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {summary_file}")
            return True

        except Exception as e:
            if not silent:
                self.console.print_error(f"æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            self.logger.error(f"æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
            return False
    
    def _parse_classification_response(self, response: str) -> Tuple[str, float, str]:
        """
        è§£æåˆ†ç±»å“åº”ï¼ˆä¸æ—§è„šæœ¬é€»è¾‘ä¸€è‡´ï¼‰

        Args:
            response: AIå“åº”å†…å®¹

        Returns:
            (åˆ†ç±»åç§°, ç½®ä¿¡åº¦, MDå†…å®¹)
        """
        category = "å¤šæ¨¡æ€ç”Ÿæˆ"  # é»˜è®¤åˆ†ç±»
        confidence = 0.8  # é»˜è®¤ç½®ä¿¡åº¦
        md_content = ""

        try:
            # æŒ‰è¡Œåˆ†å‰²å“åº”ï¼ˆä¸æ—§è„šæœ¬ä¸€è‡´ï¼‰
            lines = response.split('\n')

            # æå–åˆ†ç±»åç§°ï¼ˆç¬¬ä¸€è¡Œï¼Œä¸æ—§è„šæœ¬ä¸€è‡´ï¼‰
            if lines:
                category = lines[0].strip().replace('#', '').replace('ï¼š', '').replace(':', '').strip()
                # å»é™¤å¯èƒ½çš„ç©ºè¡Œï¼Œè·å–å®é™…çš„MDå†…å®¹
                md_content = '\n'.join(lines[1:]).strip()

            # å¦‚æœåˆ†ç±»åç§°ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not category:
                category = "å¤šæ¨¡æ€ç”Ÿæˆ"

            # å¦‚æœMDå†…å®¹ä¸ºç©ºï¼Œç”Ÿæˆé»˜è®¤å†…å®¹
            if not md_content:
                md_content = f"# æ¨¡å‹åˆ†æ\n\n**åˆ†ç±»**ï¼š{category}\n\n**è¯´æ˜**ï¼šAIåˆ†æç”Ÿæˆçš„å†…å®¹"

        except Exception as e:
            self.logger.warning(f"è§£æåˆ†ç±»å“åº”å¼‚å¸¸: {e}")
            # ä½¿ç”¨æ•´ä¸ªå“åº”ä½œä¸ºMDå†…å®¹
            md_content = response.strip() if response else "# åˆ†æå¤±è´¥\n\næ— æ³•è§£æAIå“åº”"

        return category, confidence, md_content
    
    def _generate_default_md_content(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆé»˜è®¤MDå†…å®¹
        
        Args:
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            é»˜è®¤MDå†…å®¹
        """
        return f"""# {analysis_result.title_zh}

**arXiv æ–‡ç« é“¾æ¥**ï¼š{analysis_result.url}

**ä½œè€…/å›¢é˜Ÿ**ï¼š{analysis_result.authors or 'æœªæä¾›'}

**å‘è¡¨æ—¥æœŸ**ï¼š{analysis_result.publish_date or 'æœªæä¾›'}

**æ¨¡å‹åŠŸèƒ½**ï¼š{analysis_result.model_function or 'æœªæä¾›'}

**æŠ€æœ¯ç‰¹ç‚¹**ï¼šåŸºäºè®ºæ–‡å†…å®¹çš„æŠ€æœ¯åˆ›æ–°

**åº”ç”¨åœºæ™¯**ï¼šå¤šç§å®é™…åº”ç”¨åœºæ™¯"""
    

    
    def save_classification_results(self, date: str,
                                  classification_results: List[ClassificationResult]) -> bool:
        """
        ä¿å­˜åˆ†ç±»ç»“æœåˆ°æ–‡ä»¶

        æ³¨æ„ï¼šMDæ–‡ä»¶å·²ç»åœ¨classify_and_save_single_paperæ–¹æ³•ä¸­ä¿å­˜äº†ï¼Œ
        è¿™é‡Œåªéœ€è¦ä¿å­˜åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯æˆ–å…¶ä»–æ±‡æ€»æ•°æ®

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²
            classification_results: åˆ†ç±»ç»“æœåˆ—è¡¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            date_dir = Path(self.output_dir) / date
            self.file_manager.ensure_dir(date_dir)

            # ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
            categories = {}
            for result in classification_results:
                categories[result.category] = categories.get(result.category, 0) + 1

            # ä¿å­˜åˆ†ç±»ç»Ÿè®¡åˆ°JSONæ–‡ä»¶
            stats_file = date_dir / "classification_stats.json"
            stats_data = {
                "date": date,
                "total_papers": len(classification_results),
                "categories": categories,
                "classification_time": classification_results[0].classification_time if classification_results else ""
            }

            success = self.file_manager.save_json(stats_data, stats_file)
            if success:
                self.logger.info(f"ä¿å­˜åˆ†ç±»ç»Ÿè®¡: {stats_file}")
            else:
                self.logger.error(f"ä¿å­˜åˆ†ç±»ç»Ÿè®¡å¤±è´¥: {stats_file}")

            return success

        except Exception as e:
            self.logger.error(f"ä¿å­˜åˆ†ç±»ç»“æœå¼‚å¸¸: {e}")
            return False
    
    def get_classification_statistics(self) -> Dict[str, Any]:
        """
        è·å–åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        results_dir = Path(self.output_dir)
        
        if not results_dir.exists():
            return {"total_dates": 0, "dates": []}
        
        date_dirs = [d for d in results_dir.iterdir() if d.is_dir()]
        
        return {
            "total_dates": len(date_dirs),
            "dates": [d.name for d in date_dirs],
            "results_dir": str(results_dir)
        }


class MDGenerator:
    """
    MDæ–‡ä»¶ç”Ÿæˆå™¨

    è´Ÿè´£ç”Ÿæˆå„ç§æ ¼å¼çš„Markdownæ–‡ä»¶
    """

    def __init__(self):
        """åˆå§‹åŒ–MDç”Ÿæˆå™¨"""
        self.logger = get_logger('md_generator')

    def generate_category_md(self, category: str,
                           classification_results: List[ClassificationResult],
                           date: str) -> str:
        """
        ç”Ÿæˆåˆ†ç±»MDå†…å®¹

        Args:
            category: åˆ†ç±»åç§°
            classification_results: è¯¥åˆ†ç±»çš„åˆ†ç±»ç»“æœåˆ—è¡¨
            date: æ—¥æœŸå­—ç¬¦ä¸²

        Returns:
            MDå†…å®¹å­—ç¬¦ä¸²
        """
        content = f"# {category}\n\n"
        content += f"ç”Ÿæˆæ—¥æœŸï¼š{date}\n"
        content += f"è®ºæ–‡æ•°é‡ï¼š{len(classification_results)} ç¯‡\n\n"

        # æ·»åŠ æ¯ç¯‡è®ºæ–‡çš„å†…å®¹
        for i, result in enumerate(classification_results, 1):
            content += f"## è®ºæ–‡ {i}\n\n"
            content += result.md_content
            content += "\n\n---\n\n"

        return content

    def generate_summary_md(self, summary: AnalysisSummary) -> str:
        """
        ç”Ÿæˆæ±‡æ€»MDå†…å®¹

        Args:
            summary: åˆ†ææ±‡æ€»å¯¹è±¡

        Returns:
            æ±‡æ€»MDå†…å®¹
        """
        content = f"# è®ºæ–‡åˆ†ç±»æ±‡æ€»æŠ¥å‘Š\n\n"
        content += f"ç”Ÿæˆæ—¶é—´ï¼š{summary.date}\n\n"
        content += f"## åˆ†ç±»ç»Ÿè®¡\n\n"
        content += f"- **æ€»è®ºæ–‡æ•°**ï¼š{summary.total_papers} ç¯‡\n"
        content += f"- **åˆ†ç±»æ•°é‡**ï¼š{len(summary.categories)} ä¸ª\n\n"
        content += f"## å„åˆ†ç±»è¯¦æƒ…\n\n"

        # æŒ‰è®ºæ–‡æ•°é‡æ’åº
        sorted_categories = sorted(summary.categories.items(),
                                 key=lambda x: x[1], reverse=True)

        for category, count in sorted_categories:
            percentage = count / max(summary.total_papers, 1) * 100
            content += f"### {category}\n"
            content += f"- è®ºæ–‡æ•°é‡ï¼š{count} ç¯‡\n"
            content += f"- å æ¯”ï¼š{percentage:.1f}%\n\n"

        return content

    def generate_paper_md(self, analysis_result: AnalysisResult) -> str:
        """
        ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„MDå†…å®¹

        Args:
            analysis_result: åˆ†æç»“æœ

        Returns:
            è®ºæ–‡MDå†…å®¹
        """
        content = f"# {analysis_result.translation}\n\n"
        content += f"**è®ºæ–‡æ ‡é¢˜**ï¼š{analysis_result.title}\n"
        content += f"**ä¸­æ–‡æ ‡é¢˜**ï¼š{analysis_result.translation}\n"
        content += f"**è®ºæ–‡åœ°å€**ï¼š{analysis_result.paper_url}\n\n"
        content += f"**ä½œè€…å›¢é˜Ÿ**ï¼š{analysis_result.authors}\n"
        content += f"**å‘è¡¨æ—¥æœŸ**ï¼š{analysis_result.publish_date}\n"
        content += f"**æ¨¡å‹åŠŸèƒ½**ï¼š{analysis_result.model_function}\n"

        return content


# ä¾¿æ·å‡½æ•°
def create_classifier(config: Dict[str, Any]) -> PaperClassifier:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºåˆ†ç±»å™¨å®ä¾‹

    Args:
        config: é…ç½®å­—å…¸

    Returns:
        PaperClassifierå®ä¾‹
    """
    return PaperClassifier(config)

def classify_papers(analysis_results: List[AnalysisResult], date: str = None,
                   output_dir: str = 'data/analysis_results',
                   ai_model: str = 'zhipu', silent: bool = False) -> List[ClassificationResult]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†ç±»è®ºæ–‡

    Args:
        analysis_results: åˆ†æç»“æœåˆ—è¡¨
        date: æ—¥æœŸå­—ç¬¦ä¸²
        output_dir: è¾“å‡ºç›®å½•
        ai_model: AIæ¨¡å‹ç±»å‹
        silent: æ˜¯å¦é™é»˜æ¨¡å¼

    Returns:
        åˆ†ç±»ç»“æœåˆ—è¡¨
    """
    config = {
        'output_dir': output_dir,
        'ai_model': ai_model
    }
    classifier = PaperClassifier(config)
    return classifier.classify_papers(analysis_results, date, silent)

def create_md_generator() -> MDGenerator:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºMDç”Ÿæˆå™¨å®ä¾‹

    Returns:
        MDGeneratorå®ä¾‹
    """
    return MDGenerator()
